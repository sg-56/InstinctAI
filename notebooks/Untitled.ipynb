{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfc23e6-08a6-410e-ab1c-f75bf0458f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.dataingestion import DataIngestion\n",
    "from src.components.datapreprocessing import DataFramePreprocessor\n",
    "from src.components.clustering import ClusteringEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c82519-172f-41c2-a2ec-922920d09076",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = DataIngestion()\n",
    " ## check the docstring for this on how to use this\n",
    "engine = ClusteringEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09eecdec-ab84-4ba4-938d-0c397fe18df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Union, Optional, Tuple\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# import shap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pickle\n",
    "from auto_shap.auto_shap import generate_shap_values\n",
    "\n",
    "class ClusterNode:\n",
    "    def __init__(self, level, data_indices, df, path, kpi_column=None):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.level = level\n",
    "        # Store the actual indices from the original dataframe\n",
    "        self.indices = data_indices\n",
    "        # Calculate centroid using the actual rows\n",
    "        # self.centroid = df.loc[data_indices].mean(axis=0).tolist()\n",
    "        self.size = len(data_indices)\n",
    "        self.path = path\n",
    "        self.children = []\n",
    "        self.score = None\n",
    "        self.analysis = None\n",
    "        self.kpi_column = kpi_column  # Store the KPI column this node is built for\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"level\": self.level,\n",
    "            # \"centroid\": self.centroid,\n",
    "            \"size\": self.size,\n",
    "            \"indices\": self.indices,\n",
    "            \"path\": self.path,\n",
    "            \"score\": self.score,\n",
    "            \"analysis\": self.analysis,\n",
    "            \"kpi_column\": self.kpi_column,\n",
    "            \"children\": [child.to_dict() for child in self.children]\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data, df=None):\n",
    "        \"\"\"Create a ClusterNode from a dictionary representation.\"\"\"\n",
    "        node = cls(\n",
    "            level=data[\"level\"],\n",
    "            data_indices=data[\"indices\"],\n",
    "            df=df,\n",
    "            path=data[\"path\"],\n",
    "            kpi_column=data.get(\"kpi_column\")  # Handle backward compatibility\n",
    "        )\n",
    "        node.id = data[\"id\"]\n",
    "        # node.centroid = data[\"centroid\"]\n",
    "        node.size = data[\"size\"]\n",
    "        node.score = data[\"score\"]\n",
    "        node.analysis = data[\"analysis\"]\n",
    "        \n",
    "        # Recursively create children\n",
    "        for child_data in data[\"children\"]:\n",
    "            child = cls.from_dict(child_data, df)\n",
    "            node.children.append(child)\n",
    "            \n",
    "        return node\n",
    "    \n",
    "    def save_to_json(self, filepath):\n",
    "        \"\"\"Save the cluster tree to a JSON file.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_json(cls, filepath, df=None):\n",
    "        \"\"\"Load a cluster tree from a JSON file.\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return cls.from_dict(data, df)\n",
    "    \n",
    "    def save_to_pickle(self, filepath):\n",
    "        \"\"\"Save the cluster tree to a pickle file.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_pickle(cls, filepath):\n",
    "        \"\"\"Load a cluster tree from a pickle file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "class ClusteringEngine:\n",
    "    def __init__(self, max_depth=5, min_cluster_size=10, max_k=5, discrete_numeric_threshold=25):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.max_k = max_k\n",
    "        self.discrete_numeric_threshold = discrete_numeric_threshold\n",
    "        self.original_df = None\n",
    "        self.selected_features = {}  # Dictionary to store selected features for each KPI\n",
    "        self.kpi_trees = {}  # Dictionary to store cluster trees for each KPI\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Generate a Dictionary for the clusters.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to convert\n",
    "            filepath: Path to save the JSON file\n",
    "        \"\"\"\n",
    "        if self.kpi_trees is None: \n",
    "            raise ValueError(\"No cluster trees stored. Call build_cluster_trees first.\")\n",
    "        \n",
    "        # Convert each tree to a dictionary \n",
    "        return {kpi: tree.to_dict() for kpi, tree in self.kpi_trees.items()}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def feature_selector(\n",
    "        data_frame: pd.DataFrame,\n",
    "        target_columns: Union[str, List[str]],\n",
    "        n_features: Optional[int] = None,\n",
    "        threshold: Optional[float] = None,\n",
    "        user_selected_features: Optional[List[str]] = None,\n",
    "        plot_importance: bool = False,\n",
    "        target_weights: Optional[Dict[str, float]] = None,\n",
    "        random_state: int = 42\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Select the most important features using SHAP values, supporting multiple target columns\n",
    "        and user-specified feature preferences. Task type (regression/classification) is automatically\n",
    "        determined based on the data type of the target column(s).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_frame : pandas DataFrame\n",
    "            DataFrame containing both features and target column(s)\n",
    "        target_columns : str or list of str\n",
    "            Name(s) of the target column(s) in the DataFrame\n",
    "        n_features : int, optional\n",
    "            Number of top features to select. Either n_features or threshold must be provided.\n",
    "        threshold : float, optional\n",
    "            Threshold for cumulative importance. Features are selected until their\n",
    "            cumulative importance exceeds this threshold (0-1). Either n_features or threshold must be provided.\n",
    "        user_selected_features : list of str, optional\n",
    "            List of feature names that the user believes are important and should be included\n",
    "            regardless of their SHAP importance\n",
    "        plot_importance : bool, default=True\n",
    "            Whether to plot feature importance\n",
    "        target_weights : dict, optional\n",
    "            Dictionary mapping target column names to their importance weights.\n",
    "            If not provided, all targets are weighted equally.\n",
    "        random_state : int, default=42\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        selected_features : list\n",
    "            List of selected feature names\n",
    "        \"\"\"\n",
    "        # Convert target_columns to list if it's a single string\n",
    "        if isinstance(target_columns, str):\n",
    "            target_columns = [target_columns]\n",
    "        \n",
    "        # Input validation\n",
    "        for col in target_columns:\n",
    "            if col not in data_frame.columns:\n",
    "                raise ValueError(f\"Target column '{col}' does not exist in the data frame\")\n",
    "        \n",
    "        if n_features is None and threshold is None:\n",
    "            raise ValueError(\"Either n_features or threshold must be provided\")\n",
    "        \n",
    "        if threshold is not None and (threshold <= 0 or threshold > 1):\n",
    "            raise ValueError(\"Threshold must be between 0 and 1\")\n",
    "        \n",
    "        if n_features is not None and n_features <= 0:\n",
    "            raise ValueError(\"n_features must be positive\")\n",
    "        \n",
    "        # Separate features and targets\n",
    "        X = data_frame.drop(columns=target_columns)\n",
    "        \n",
    "        if n_features is not None and n_features > X.shape[1]:\n",
    "            n_features = X.shape[1]\n",
    "            print(f\"Warning: n_features was greater than the number of available features. Setting to {n_features}\")\n",
    "        \n",
    "        # Validate user_selected_features\n",
    "        if user_selected_features is not None:\n",
    "            # Convert to set for faster lookup\n",
    "            user_features_set = set(user_selected_features)\n",
    "            invalid_features = user_features_set - set(X.columns)\n",
    "            if invalid_features:\n",
    "                raise ValueError(f\"The following user-selected features are not in the dataset: {invalid_features}\")\n",
    "            \n",
    "            # Check if user selected all features\n",
    "            if len(user_features_set) == X.shape[1]:\n",
    "                print(\"User selected all features. No SHAP-based selection will be performed.\")\n",
    "                return list(X.columns)\n",
    "        else:\n",
    "            user_features_set = set()\n",
    "        \n",
    "        # Determine task type for each target column\n",
    "        task_types = {}\n",
    "        for col in target_columns:\n",
    "            # Check if the column contains categorical data\n",
    "            unique_values = data_frame[col].nunique()\n",
    "            if pd.api.types.is_numeric_dtype(data_frame[col]):\n",
    "                # For numeric types, if there are few unique values and they're all integers, it's likely classification\n",
    "                if unique_values <= 10 and np.array_equal(data_frame[col], data_frame[col].astype(int)):\n",
    "                    task_types[col] = 'classification'\n",
    "                else:\n",
    "                    task_types[col] = 'regression'\n",
    "            else:\n",
    "                # Non-numeric types are always classification\n",
    "                task_types[col] = 'classification'\n",
    "        \n",
    "        # Print determined task types\n",
    "        print(\"Automatically determined task types:\")\n",
    "        for col, task_type in task_types.items():\n",
    "            print(f\"- {col}: {task_type}\")\n",
    "        \n",
    "        # Handle target weights\n",
    "        if target_weights is None:\n",
    "            # Equal weights for all targets\n",
    "            target_weights = {name: 1.0/len(target_columns) for name in target_columns}\n",
    "        else:\n",
    "            # Validate target weights\n",
    "            missing_targets = set(target_columns) - set(target_weights.keys())\n",
    "            if missing_targets:\n",
    "                warnings.warn(f\"Target weights not provided for: {missing_targets}. Using default weight of 0.\")\n",
    "                for target in missing_targets:\n",
    "                    target_weights[target] = 0.0\n",
    "                    \n",
    "            # Normalize weights to sum to 1\n",
    "            weight_sum = sum(target_weights.values())\n",
    "            if weight_sum == 0:\n",
    "                raise ValueError(\"Sum of target weights cannot be zero\")\n",
    "            target_weights = {k: v/weight_sum for k, v in target_weights.items()}\n",
    "        \n",
    "        print(f\"Processing {len(target_columns)} target{'s' if len(target_columns) > 1 else ''}\")\n",
    "        for target, weight in target_weights.items():\n",
    "            print(f\"- {target}: weight = {weight:.3f}\")\n",
    "        \n",
    "        # Calculate feature importance for each target\n",
    "        feature_importance_dict = {}\n",
    "        shap_values_dict = {}\n",
    "        \n",
    "        for target_name in target_columns:\n",
    "            if target_weights.get(target_name, 0) == 0:\n",
    "                print(f\"Skipping target '{target_name}' with weight 0\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Computing feature importance for target: {target_name}\")\n",
    "            \n",
    "            # Get target values\n",
    "            y = data_frame[target_name]\n",
    "            \n",
    "            # Get task type for this target\n",
    "            task_type = task_types[target_name]\n",
    "            \n",
    "            # Initialize model based on task type\n",
    "            if task_type == 'regression':\n",
    "                model = RandomForestRegressor(n_estimators=100, n_jobs = -1, random_state=random_state)\n",
    "            elif task_type == 'classification':\n",
    "                model = RandomForestClassifier(n_estimators=100,n_jobs = -1, random_state=random_state)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid task type '{task_type}' for target '{target_name}'\")\n",
    "            \n",
    "            # Fit the model\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            # Create explainer and compute SHAP values\n",
    "            _,_,shap_values = generate_shap_values(model,X)\n",
    "            print(\"TARGET: \",target_name,shap_values)\n",
    "\n",
    "            # if len(shap_values.shape) == 3:\n",
    "            #     shap_values = shap_values[:,:,-1]\n",
    "            # shap_values = shap_values[:,:,-1]\n",
    "            \n",
    "            # For classification tasks with multiple classes, shap_values will be a list\n",
    "            if isinstance(shap_values, list):\n",
    "                # Sum across all classes\n",
    "                shap_values = np.abs(np.array(shap_values)).sum(axis=0)\n",
    "            \n",
    "            # Store for later use\n",
    "            shap_values_dict[target_name] = shap_values['shap_value'].values\n",
    "            \n",
    "            # Calculate feature importance as mean absolute SHAP value for each feature\n",
    "            feature_importance_dict[target_name] = shap_values.set_index('feature').to_dict()['shap_value']\n",
    "            \n",
    "        # Combine feature importance from all targets using weights\n",
    "        combined_importance = np.zeros(X.shape[1])\n",
    "        for target_name, importance in feature_importance_dict.items():\n",
    "            weight = target_weights.get(target_name, 0)\n",
    "            # Get importance values in the same order as X.columns\n",
    "            importance_values = np.array([importance.get(col, 0) for col in X.columns])\n",
    "            combined_importance += importance_values * weight\n",
    "        \n",
    "        # Create DataFrame with feature names and importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': combined_importance,\n",
    "            'User_Selected': [feature in user_features_set for feature in X.columns]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Also create target-specific importance dataframes\n",
    "        target_importance_dfs = {}\n",
    "        for target_name, importance in feature_importance_dict.items():\n",
    "            # Create importance array in same order as X.columns\n",
    "            importance_values = np.array([importance.get(col, 0) for col in X.columns])\n",
    "            target_importance_dfs[target_name] = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Importance': importance_values\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Start with user-selected features\n",
    "        selected_features = list(user_features_set)\n",
    "        \n",
    "        # Add features based on SHAP importance (excluding those already selected by user)\n",
    "        remaining_features = feature_importance_df[~feature_importance_df['User_Selected']]\n",
    "        \n",
    "        # Calculate how many additional features to select (if using n_features)\n",
    "        if n_features is not None:\n",
    "            remaining_n = max(0, n_features - len(selected_features))\n",
    "            additional_features = remaining_features['Feature'].tolist()[:remaining_n]\n",
    "            selected_features.extend(additional_features)\n",
    "        else:\n",
    "            # For threshold-based selection, we need to recalculate importance considering already selected features\n",
    "            if selected_features:\n",
    "                # Calculate the importance of already selected features\n",
    "                selected_importance = feature_importance_df[feature_importance_df['User_Selected']]['Importance'].sum()\n",
    "                total_importance = feature_importance_df['Importance'].sum()\n",
    "                \n",
    "                # Calculate the remaining importance needed\n",
    "                remaining_importance_needed = min(threshold * total_importance - selected_importance, \n",
    "                                                remaining_features['Importance'].sum())\n",
    "                \n",
    "                if remaining_importance_needed > 0:\n",
    "                    # Calculate cumulative importance for remaining features\n",
    "                    remaining_features['Cumulative_Importance'] = remaining_features['Importance'].cumsum()\n",
    "                    \n",
    "                    # Select features until we reach the needed importance\n",
    "                    additional_features = remaining_features[remaining_features['Cumulative_Importance'] <= remaining_importance_needed]['Feature'].tolist()\n",
    "                    selected_features.extend(additional_features)\n",
    "            else:\n",
    "                # If no user-selected features, proceed with normal threshold selection\n",
    "                remaining_features['Cumulative_Importance'] = remaining_features['Importance'].cumsum() / feature_importance_df['Importance'].sum()\n",
    "                additional_features = remaining_features[remaining_features['Cumulative_Importance'] <= threshold]['Feature'].tolist()\n",
    "                selected_features.extend(additional_features)\n",
    "        \n",
    "        # If no features were selected, take at least the most important one\n",
    "        if not selected_features:\n",
    "            selected_features = [feature_importance_df['Feature'].iloc[0]]\n",
    "        \n",
    "        print(f\"Selected {len(selected_features)}/{X.shape[1]} features\")\n",
    "        print(f\"- User-selected features: {len(user_features_set)}\")\n",
    "        print(f\"- SHAP-selected features: {len(selected_features) - len(user_features_set)}\")\n",
    "        \n",
    "        # Store target-specific feature importance for potential individual KPI tree building\n",
    "        feature_importance_per_target = {\n",
    "            target: target_df.sort_values('Importance', ascending=False)\n",
    "            for target, target_df in target_importance_dfs.items()\n",
    "        }\n",
    "        \n",
    "        return selected_features, feature_importance_per_target\n",
    "\n",
    "    def _best_k_by_silhouette(self, df):\n",
    "        best_k = 2\n",
    "        best_score = -1\n",
    "        X = df.to_numpy()\n",
    "\n",
    "        for k in range(2, min(self.max_k + 1, len(df))):\n",
    "            try:\n",
    "                labels = GaussianMixture(n_components=k, covariance_type='full', random_state=0).fit_predict(X)\n",
    "                score = silhouette_score(X, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_k = k\n",
    "            except Exception as e:\n",
    "                print(f\"Error during clustering with k={k}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return best_k\n",
    "\n",
    "    def _build_tree(self, df, indices, level, path_prefix, perform_analysis=True, columns_to_analyze=None, kpi_column=None):\n",
    "        \"\"\"\n",
    "        Build the cluster tree recursively.\n",
    "        \n",
    "        Args:\n",
    "            df: The full DataFrame\n",
    "            indices: List of indices from the original DataFrame for this cluster\n",
    "            level: Current depth level\n",
    "            path_prefix: Path prefix for this node\n",
    "            perform_analysis: Whether to perform analysis\n",
    "            columns_to_analyze: Columns to analyze\n",
    "            kpi_column: The KPI column this tree is built for\n",
    "            \n",
    "        Returns:\n",
    "            ClusterNode: The created node\n",
    "        \"\"\"\n",
    "        # Create node with the provided indices (which are the actual DataFrame indices)\n",
    "        node = ClusterNode(level, indices, df, path=[path_prefix], kpi_column=kpi_column)\n",
    "\n",
    "        # Perform dataset comparison analysis\n",
    "        if perform_analysis and columns_to_analyze:\n",
    "            full_df = df.copy()\n",
    "            segment_df = df.loc[indices].copy()\n",
    "            node.analysis = self.compare_datasets(\n",
    "                full_df=full_df,\n",
    "                segment_df=segment_df,\n",
    "                columns_to_analyze=columns_to_analyze\n",
    "            )\n",
    "\n",
    "        # Terminal case: reached max depth or min cluster size\n",
    "        if level >= self.max_depth or len(indices) < self.min_cluster_size:\n",
    "            return node\n",
    "\n",
    "        # Get the subset of data for this cluster\n",
    "        sub_df = df.loc[indices]\n",
    "        \n",
    "        # Find optimal number of clusters\n",
    "        k = self._best_k_by_silhouette(sub_df)\n",
    "        \n",
    "        # Perform clustering on the subset\n",
    "        agg_clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "        cluster_labels = agg_clustering.fit_predict(sub_df)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        node.score = silhouette_score(sub_df, cluster_labels)\n",
    "\n",
    "        # Create a mapping from position in sub_df to actual index in original df\n",
    "        position_to_index = {i: idx for i, idx in enumerate(sub_df.index)}\n",
    "\n",
    "        # Process each cluster\n",
    "        for cluster_id in range(k):\n",
    "            # Find positions where the cluster label matches\n",
    "            cluster_positions = np.where(cluster_labels == cluster_id)[0]\n",
    "            \n",
    "            # Map these positions back to actual DataFrame indices\n",
    "            cluster_indices = [position_to_index[pos] for pos in cluster_positions]\n",
    "            \n",
    "            if len(cluster_indices) >= self.min_cluster_size:\n",
    "                child_path = f\"{path_prefix}_{cluster_id}\"\n",
    "                child_node = self._build_tree(\n",
    "                    df, \n",
    "                    cluster_indices, \n",
    "                    level + 1, \n",
    "                    child_path, \n",
    "                    perform_analysis, \n",
    "                    columns_to_analyze,\n",
    "                    kpi_column\n",
    "                )\n",
    "                child_node.path = node.path + [child_path]\n",
    "                node.children.append(child_node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def build_cluster_trees(self, df, columns_to_analyze=None, kpi_columns=None):\n",
    "        \"\"\"\n",
    "        Build separate cluster trees for each KPI column.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to cluster\n",
    "            columns_to_analyze: List of columns to analyze in each cluster\n",
    "            kpi_columns: List of KPI columns to build trees for\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping KPI columns to their respective cluster tree root nodes\n",
    "        \"\"\"\n",
    "        # Store the original dataframe for later retrieval\n",
    "        self.original_df = df.copy()\n",
    "        \n",
    "        if columns_to_analyze is None:\n",
    "            columns_to_analyze = df.columns.tolist()\n",
    "            \n",
    "        if kpi_columns is None or len(kpi_columns) == 0:\n",
    "            raise ValueError(\"At least one KPI column must be provided\")\n",
    "            \n",
    "        # Initialize the dictionary to store trees for each KPI\n",
    "        self.kpi_trees = {}\n",
    "            \n",
    "        # For each KPI column, build a separate cluster tree\n",
    "        for kpi_column in kpi_columns:\n",
    "            print(f\"Building cluster tree for KPI: {kpi_column}\")\n",
    "            \n",
    "            # Select features specifically for this KPI\n",
    "            features, feature_importances = self.feature_selector(\n",
    "                data_frame=df,\n",
    "                target_columns=[kpi_column],  # Only this KPI\n",
    "                user_selected_features=columns_to_analyze,\n",
    "                threshold=0.3\n",
    "            )\n",
    "            \n",
    "            self.selected_features[kpi_column] = features\n",
    "            \n",
    "            # Use all indices from the original DataFrame\n",
    "            indices = df.index.tolist()\n",
    "            \n",
    "            # Build tree for this specific KPI\n",
    "            root_node = self._build_tree(\n",
    "                df[features], \n",
    "                indices, \n",
    "                level=0, \n",
    "                path_prefix=f\"root_{kpi_column}\", \n",
    "                perform_analysis=True, \n",
    "                columns_to_analyze=columns_to_analyze,\n",
    "                kpi_column=kpi_column\n",
    "            )\n",
    "            \n",
    "            # Store the tree\n",
    "            self.kpi_trees[kpi_column] = root_node\n",
    "            \n",
    "        return self.kpi_trees\n",
    "\n",
    "    def refine_cluster(self, df, indices, n_neighbors=50, columns_to_analyze=None, kpi_column=None):\n",
    "        \"\"\"\n",
    "        Refine a cluster using nearest neighbors.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the data\n",
    "            indices: Indices of the data points to refine\n",
    "            n_neighbors: Number of neighbors to consider\n",
    "            columns_to_analyze: List of columns to analyze in the refined cluster\n",
    "            kpi_column: The KPI column this refined cluster is for\n",
    "            \n",
    "        Returns:\n",
    "            The root node of the refined cluster tree\n",
    "        \"\"\"\n",
    "        # Store the original dataframe for later retrieval if not already stored\n",
    "        if self.original_df is None:\n",
    "            self.original_df = df.copy()\n",
    "            \n",
    "        if columns_to_analyze is None:\n",
    "            columns_to_analyze = df.columns.tolist()\n",
    "            \n",
    "        sub_df = df.loc[indices]\n",
    "        X = sub_df.to_numpy()\n",
    "        \n",
    "        # Use at most the number of points we have\n",
    "        real_n_neighbors = min(n_neighbors, len(sub_df))\n",
    "        nbrs = NearestNeighbors(n_neighbors=real_n_neighbors).fit(X)\n",
    "        _, neighbor_indices = nbrs.kneighbors(X)\n",
    "        \n",
    "        # Map neighbor indices (which are positions in X) back to indices in the sub_df\n",
    "        neighbor_positions = list(set(neighbor_indices.flatten()))\n",
    "        \n",
    "        # Map from positions in sub_df to actual indices in original df\n",
    "        position_to_orig_index = {i: idx for i, idx in enumerate(sub_df.index)}\n",
    "        \n",
    "        # Get the actual DataFrame indices for these neighbors\n",
    "        selected_global_indices = [position_to_orig_index[pos] for pos in neighbor_positions]\n",
    "\n",
    "        return self._build_tree(\n",
    "            df, \n",
    "            selected_global_indices, \n",
    "            level=0, \n",
    "            path_prefix=\"refined\", \n",
    "            perform_analysis=True, \n",
    "            columns_to_analyze=columns_to_analyze,\n",
    "            kpi_column=kpi_column\n",
    "        )\n",
    "\n",
    "    # Node retrieval methods\n",
    "    def get_node_by_level_or_path(self, root_node: ClusterNode, level: Optional[int] = None, \n",
    "                                path: Optional[List[str]] = None) -> List[ClusterNode]:\n",
    "        \"\"\"\n",
    "        Find cluster nodes by level and/or path.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            level: The level to search for (if None, ignores level constraint)\n",
    "            path: The path to search for (if None, ignores path constraint)\n",
    "            \n",
    "        Returns:\n",
    "            List of matching cluster nodes\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        \n",
    "        def _traverse(node):\n",
    "            # Check if node matches criteria\n",
    "            level_match = level is None or node.level == level\n",
    "            path_match = path is None or self._path_matches(node.path, path)\n",
    "            \n",
    "            if level_match and path_match:\n",
    "                result.append(node)\n",
    "            \n",
    "            # Continue traversing children\n",
    "            for child in node.children:\n",
    "                _traverse(child)\n",
    "        \n",
    "        _traverse(root_node)\n",
    "        return result\n",
    "    \n",
    "    def _path_matches(self, node_path: List[str], search_path: List[str]) -> bool:\n",
    "        \"\"\"Check if a node's path matches or contains the search path.\"\"\"\n",
    "        if len(search_path) > len(node_path):\n",
    "            return False\n",
    "        \n",
    "        # Check if the beginning of node_path matches search_path\n",
    "        for i in range(len(search_path)):\n",
    "            if search_path[i] != node_path[i]:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def get_node_dataframe(self, node: ClusterNode) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the DataFrame corresponding to a specific cluster node.\n",
    "        \n",
    "        Args:\n",
    "            node: The cluster node to extract data for\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the data points in the cluster\n",
    "        \"\"\"\n",
    "        if self.original_df is None:\n",
    "            raise ValueError(\"No original DataFrame is stored. Call build_cluster_trees first.\")\n",
    "        \n",
    "        # Use node.indices which contain actual DataFrame indices\n",
    "        return self.original_df.loc[node.indices].copy()\n",
    "    \n",
    "    def get_dataframes_by_level_or_path(self, root_node: ClusterNode, level: Optional[int] = None, \n",
    "                                      path: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get DataFrames for nodes matching specified level and/or path.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            level: The level to search for (if None, ignores level constraint)\n",
    "            path: The path to search for (if None, ignores path constraint)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping node IDs to their corresponding DataFrames\n",
    "        \"\"\"\n",
    "        matching_nodes = self.get_node_by_level_or_path(root_node, level, path)\n",
    "        \n",
    "        result = {}\n",
    "        for node in matching_nodes:\n",
    "            node_df = self.get_node_dataframe(node)\n",
    "            # Use node path as the key, converted to a string for dictionary key\n",
    "            key = f\"level_{node.level}_{'_'.join(node.path)}\"\n",
    "            result[key] = node_df\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def find_node_by_exact_path(self, root_node: ClusterNode, path: List[str]) -> Optional[ClusterNode]:\n",
    "        \"\"\"\n",
    "        Find a specific node by its exact path.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            path: The exact path to find\n",
    "            \n",
    "        Returns:\n",
    "            The matching node or None if not found\n",
    "        \"\"\"\n",
    "        def _find(node):\n",
    "            if node.path == path:\n",
    "                return node\n",
    "            \n",
    "            for child in node.children:\n",
    "                result = _find(child)\n",
    "                if result:\n",
    "                    return result\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        return _find(root_node)\n",
    "    \n",
    "    def get_dataframe_by_path_string(self, root_node: ClusterNode, path_string: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame for a node specified by a path string like \"root_0_1\".\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            path_string: A string representing the path, with components separated by underscores\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame for the specified node\n",
    "        \"\"\"\n",
    "        path = path_string.split('_')\n",
    "        node = self.find_node_by_exact_path(root_node, path)\n",
    "        \n",
    "        if node:\n",
    "            return self.get_node_dataframe(node)\n",
    "        else:\n",
    "            raise ValueError(f\"No node found with path: {path_string}\")\n",
    "    \n",
    "    def get_labeled_dataframe(self, kpi_column=None, level: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the original DataFrame with cluster labels added.\n",
    "        \n",
    "        Args:\n",
    "            kpi_column: The specific KPI column's tree to get labels from (if None, includes all KPI trees)\n",
    "            level: The specific level to get labels for (if None, includes all levels)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cluster labels as additional columns\n",
    "        \"\"\"\n",
    "        if self.original_df is None or not self.kpi_trees:\n",
    "            raise ValueError(\"No cluster trees stored. Call build_cluster_trees first.\")\n",
    "        \n",
    "        # Start with a copy of the original dataframe\n",
    "        result_df = self.original_df.copy()\n",
    "        \n",
    "        # If specific KPI column provided, only process that tree\n",
    "        kpi_trees_to_process = {kpi_column: self.kpi_trees[kpi_column]} if kpi_column in self.kpi_trees else self.kpi_trees\n",
    "        \n",
    "        # Process each KPI tree\n",
    "        for kpi, root_node in kpi_trees_to_process.items():\n",
    "            # Create a mapping of index to path for each level\n",
    "            index_to_path_by_level = {}\n",
    "            \n",
    "            def _collect_paths(node):\n",
    "                if level is None or node.level == level:\n",
    "                    node_level = node.level\n",
    "                    if node_level not in index_to_path_by_level:\n",
    "                        index_to_path_by_level[node_level] = {}\n",
    "                    \n",
    "                    path_str = '_'.join(node.path)\n",
    "                    for idx in node.indices:\n",
    "                        index_to_path_by_level[node_level][idx] = path_str\n",
    "                \n",
    "                for child in node.children:\n",
    "                    _collect_paths(child)\n",
    "            \n",
    "            _collect_paths(root_node)\n",
    "            \n",
    "            # Add columns for each level\n",
    "            for lvl in sorted(index_to_path_by_level.keys()):\n",
    "                col_name = f\"cluster_{kpi}_level_{lvl}\"\n",
    "                result_df[col_name] = pd.Series(index_to_path_by_level[lvl])\n",
    "                \n",
    "        return result_df\n",
    "\n",
    "    # Get all clusters at a specific level for a specific KPI\n",
    "    def get_all_clusters_at_level(self, kpi_column, level: int) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get all clusters at a specific level for a specific KPI as a dictionary of DataFrames.\n",
    "        \n",
    "        Args:\n",
    "            kpi_column: The KPI column to get clusters for\n",
    "            level: The level to get clusters from\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping cluster paths to their DataFrames\n",
    "        \"\"\"\n",
    "        if kpi_column not in self.kpi_trees:\n",
    "            raise ValueError(f\"No cluster tree for KPI column: {kpi_column}\")\n",
    "            \n",
    "        root_node = self.kpi_trees[kpi_column]\n",
    "        nodes = self.get_node_by_level_or_path(root_node, level=level)\n",
    "        \n",
    "        result = {}\n",
    "        for node in nodes:\n",
    "            path_str = '_'.join(node.path)\n",
    "            result[path_str] = self.get_node_dataframe(node)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # Dataset comparison functions\n",
    "    def compare_datasets(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        columns_to_analyze: List[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compare a segment to the full dataset across specified columns.\n",
    "        \n",
    "        Args:\n",
    "            full_df: The complete dataset\n",
    "            segment_df: A segment/subset of the dataset\n",
    "            columns_to_analyze: Columns to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        if segment_df.empty:\n",
    "            return {\"error\": \"Segmented dataset is empty\"}\n",
    "        \n",
    "        result = {}\n",
    "        \n",
    "        for col in columns_to_analyze:\n",
    "            if col not in full_df.columns:\n",
    "                result[col] = {\"error\": f\"Column '{col}' not found in dataset\"}\n",
    "                continue\n",
    "                \n",
    "            is_numeric = pd.api.types.is_numeric_dtype(full_df[col])\n",
    "            \n",
    "            if is_numeric:\n",
    "                unique_count = len(full_df[col].dropna().unique())\n",
    "                \n",
    "                if unique_count <= self.discrete_numeric_threshold:\n",
    "                    result[col] = self.analyze_discrete_numeric_column(full_df, segment_df, col)\n",
    "                else:\n",
    "                    result[col] = self.analyze_numeric_column(full_df, segment_df, col)\n",
    "            else:\n",
    "                result[col] = self.analyze_categorical_column(full_df, segment_df, col)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def analyze_categorical_column(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        col: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a categorical column, comparing segment to full dataset.\n",
    "        \"\"\"\n",
    "        full_values = set(full_df[col].dropna().unique())\n",
    "        segment_values = set(segment_df[col].dropna().unique())\n",
    "        categories_in_both = full_values.intersection(segment_values)\n",
    "        full_unique_count = len(full_values)\n",
    "        segment_unique_count = len(segment_values)\n",
    "        coverage_percentage = round((len(categories_in_both) / full_unique_count) * 100, 2) if full_unique_count > 0 else 0\n",
    "\n",
    "        segment_counts = segment_df[col].value_counts()\n",
    "        total_segment_count = segment_df[col].dropna().shape[0]  \n",
    "\n",
    "        mode = segment_counts.index[0] if not segment_counts.empty else None\n",
    "        mode_count = segment_counts.iloc[0] if not segment_counts.empty else 0\n",
    "\n",
    "        top_count = min(3, len(segment_counts))\n",
    "        top_categories = []\n",
    "        for i in range(top_count):\n",
    "            if i < len(segment_counts):\n",
    "                category = segment_counts.index[i]\n",
    "                count = segment_counts.iloc[i]\n",
    "                percentage = round((count / total_segment_count) * 100, 2) if total_segment_count > 0 else 0\n",
    "                top_categories.append({\"category\": str(category), \"percentage\": percentage})\n",
    "\n",
    "        bottom_count = min(3, len(segment_counts))\n",
    "        bottom_categories = []\n",
    "        for i in range(1, bottom_count + 1):\n",
    "            if len(segment_counts) >= i:\n",
    "                category = segment_counts.index[-i]\n",
    "                count = segment_counts.iloc[-i]\n",
    "                percentage = round((count / total_segment_count) * 100, 2) if total_segment_count > 0 else 0\n",
    "                bottom_categories.append({\"category\": str(category), \"percentage\": percentage})\n",
    "        \n",
    "        return {\n",
    "            \"unique_categories\": {\n",
    "                \"segment\": segment_unique_count,\n",
    "                \"full\": full_unique_count,\n",
    "                \"coverage_percentage\": coverage_percentage\n",
    "            },\n",
    "            \"mode\": {\n",
    "                \"category\": str(mode) if mode is not None else None,\n",
    "                \"count\": int(mode_count)\n",
    "            },\n",
    "            \"top_categories\": top_categories,\n",
    "            \"bottom_categories\": bottom_categories\n",
    "        }\n",
    "\n",
    "    def analyze_discrete_numeric_column(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        col: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a discrete numeric column, combining categorical and numeric analysis.\n",
    "        \"\"\"\n",
    "        categorical_analysis = self.analyze_categorical_column(full_df, segment_df, col)\n",
    "        \n",
    "        full_mean = full_df[col].mean()\n",
    "        full_sum = full_df[col].sum()\n",
    "        segment_mean = segment_df[col].mean()\n",
    "        segment_sum = segment_df[col].sum()\n",
    "        \n",
    "        sum_contribution = (segment_sum / full_sum * 100) if full_sum != 0 else 0\n",
    "        mean_contribution = (segment_mean / full_mean * 100) if full_mean != 0 else 0\n",
    "        \n",
    "        categorical_analysis[\"numeric_stats\"] = {\n",
    "            \"full_dataset\": {\n",
    "                \"mean\": float(full_mean),\n",
    "                \"sum\": float(full_sum)\n",
    "            },\n",
    "            \"segment\": {\n",
    "                \"mean\": float(segment_mean),\n",
    "                \"sum\": float(segment_sum)\n",
    "            },\n",
    "            \"contributions\": {\n",
    "                \"sum_contribution_percentage\": round(sum_contribution, 2),\n",
    "                \"mean_contribution_percentage\": round(mean_contribution, 2)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        all_values = {}\n",
    "        for value in sorted(full_df[col].dropna().unique()):\n",
    "            full_count = len(full_df[full_df[col] == value])\n",
    "            segment_count = len(segment_df[segment_df[col] == value])\n",
    "            percentage = round((segment_count / full_count * 100), 2) if full_count > 0 else 0\n",
    "            \n",
    "            if percentage > 50:\n",
    "                all_values[str(value)] = {\n",
    "                    \"full_count\": full_count,\n",
    "                    \"segment_count\": segment_count,\n",
    "                    \"percentage\": percentage\n",
    "                }\n",
    "        \n",
    "        categorical_analysis[\"value_distribution\"] = all_values\n",
    "        \n",
    "        return categorical_analysis\n",
    "\n",
    "    def analyze_numeric_column(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        col: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a continuous numeric column.\n",
    "        \"\"\"\n",
    "        full_mean = full_df[col].mean()\n",
    "        full_sum = full_df[col].sum()\n",
    "        \n",
    "        segment_mean = segment_df[col].mean()\n",
    "        segment_sum = segment_df[col].sum()\n",
    "        \n",
    "        sum_contribution = (segment_sum / full_sum * 100) if full_sum != 0 else 0\n",
    "        mean_contribution = (segment_mean / full_mean * 100) if full_mean != 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"full_dataset\": {\n",
    "                \"mean\": float(full_mean),\n",
    "                \"sum\": float(full_sum)\n",
    "            },\n",
    "            \"segment\": {\n",
    "                \"mean\": float(segment_mean),\n",
    "                \"sum\": float(segment_sum)\n",
    "            },\n",
    "            \"contributions\": {\n",
    "                \"sum_contribution_percentage\": round(sum_contribution, 2),\n",
    "                \"mean_contribution_percentage\": round(mean_contribution, 2)\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8081bc-d998-4abe-ad8b-0489f162a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.ingest_from_file('data/cleaned_apar.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca08fd40-f477-432f-a184-5b14b91a6fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Invoice No': 'int64',\n",
       " 'Line Item': 'int64',\n",
       " 'Month': 'object',\n",
       " 'Order Date': 'object',\n",
       " 'Order Type': 'object',\n",
       " 'Material': 'object',\n",
       " 'Mat. Grp Desc': 'object',\n",
       " 'Batch No': 'object',\n",
       " 'Base UoM': 'object',\n",
       " 'Qty in Sales Unit': 'float64',\n",
       " 'Sales Unit': 'object',\n",
       " 'Sales Qty': 'float64',\n",
       " 'Frieght Charged': 'float64',\n",
       " 'Freight Incurred': 'float64',\n",
       " 'Interest': 'float64',\n",
       " 'Net Value /KL (NET_VAL_KL)': 'float64',\n",
       " 'Packing Cost /KL': 'float64',\n",
       " 'Total Packing Cost': 'float64',\n",
       " 'RM Cost /KL': 'float64',\n",
       " 'Total RM Cost': 'float64',\n",
       " 'Total Variable Cost': 'float64',\n",
       " 'Total Value': 'float64',\n",
       " 'NET_CONT_KL': 'float64',\n",
       " 'DENSITY': 'float64',\n",
       " 'Sales District': 'object',\n",
       " 'Region': 'int64',\n",
       " 'REG_DESC': 'object',\n",
       " 'Payer': 'object',\n",
       " 'Customer': 'object',\n",
       " 'Employee': 'object',\n",
       " 'Sales manager': 'object',\n",
       " 'Industry Code': 'object',\n",
       " 'Customer Grp': 'object',\n",
       " 'Industry Desc': 'object',\n",
       " 'MATKL': 'object',\n",
       " 'Mat Grp1 Text': 'object',\n",
       " 'Mat Grp 2 Text': 'object',\n",
       " 'City': 'object',\n",
       " 'Doc Curr': 'object',\n",
       " 'Plant': 'int64',\n",
       " 'Plant Desc': 'object',\n",
       " 'CGST': 'float64',\n",
       " 'SGST': 'float64',\n",
       " 'UGST': 'float64',\n",
       " 'IGST': 'float64',\n",
       " 'Commission_N': 'int64',\n",
       " 'Basic Sale Price': 'float64',\n",
       " 'Credit Days': 'int64',\n",
       " 'Discount': 'int64',\n",
       " 'Revenue': 'float64'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##lists all columns along with datatypes\n",
    "ingestor.get_column_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2a7037-ac7c-4eeb-8115-2998ef0131dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns_with_missing_values': {},\n",
       " 'mixed_data_types': {},\n",
       " 'constant_value_columns': ['Line Item',\n",
       "  'Order Type',\n",
       "  'Base UoM',\n",
       "  'Frieght Charged',\n",
       "  'MATKL',\n",
       "  'Doc Curr',\n",
       "  'UGST',\n",
       "  'Commission_N',\n",
       "  'Discount'],\n",
       " 'duplicate_rows': {'count': 0, 'rows': []}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c6acac0-56d9-4a3d-b2e1-c30d7f33952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Line Item',\n",
       " 'Order Type',\n",
       " 'Base UoM',\n",
       " 'Frieght Charged',\n",
       " 'MATKL',\n",
       " 'Doc Curr',\n",
       " 'UGST',\n",
       " 'Commission_N',\n",
       " 'Discount']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.constant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe305fca-3e47-44f2-8b3f-cba68de32fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice No</th>\n",
       "      <th>Line Item</th>\n",
       "      <th>Month</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Order Type</th>\n",
       "      <th>Material</th>\n",
       "      <th>Mat. Grp Desc</th>\n",
       "      <th>Batch No</th>\n",
       "      <th>Base UoM</th>\n",
       "      <th>Qty in Sales Unit</th>\n",
       "      <th>...</th>\n",
       "      <th>Plant Desc</th>\n",
       "      <th>CGST</th>\n",
       "      <th>SGST</th>\n",
       "      <th>UGST</th>\n",
       "      <th>IGST</th>\n",
       "      <th>Commission_N</th>\n",
       "      <th>Basic Sale Price</th>\n",
       "      <th>Credit Days</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9501684612</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO60209</td>\n",
       "      <td>POWEROIL TO 335H UNINHI TYPE II 18GE209L</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>85.000</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1456730.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9501684613</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>ABUTO60000</td>\n",
       "      <td>POWEROIL TO 335H UNINHIBIT TYPE II BULK</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>11.973</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>78123.83</td>\n",
       "      <td>78123.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1000</td>\n",
       "      <td>72500.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>868042.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9501684614</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO60209</td>\n",
       "      <td>POWEROIL TO 335H UNINHI TYPE II 18GE209L</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>85.000</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1456730.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9501684619</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>ABUTO60000</td>\n",
       "      <td>POWEROIL TO 335H UNINHIBIT TYPE II BULK</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>24.952</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>159443.28</td>\n",
       "      <td>159443.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1771592.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9501684620</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO60209</td>\n",
       "      <td>POWEROIL TO 335H UNINHI TYPE II 18GE209L</td>\n",
       "      <td>RIT0539L23</td>\n",
       "      <td>KL</td>\n",
       "      <td>32.000</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>52367.04</td>\n",
       "      <td>52367.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>87000.0</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>581856.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>9503659135</td>\n",
       "      <td>10</td>\n",
       "      <td>Mar 2024</td>\n",
       "      <td>2024-03-30</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>ABUTO60000</td>\n",
       "      <td>POWEROIL TO 335H UNINHIBIT TYPE II BULK</td>\n",
       "      <td>SIT0020324</td>\n",
       "      <td>KL</td>\n",
       "      <td>25.006</td>\n",
       "      <td>...</td>\n",
       "      <td>SILVASSA</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>339831.54</td>\n",
       "      <td>100</td>\n",
       "      <td>75500.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1887953.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>9503659137</td>\n",
       "      <td>10</td>\n",
       "      <td>Mar 2024</td>\n",
       "      <td>2024-03-30</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO60209</td>\n",
       "      <td>POWEROIL TO 335H UNINHI TYPE II 18GE209L</td>\n",
       "      <td>SIT0020324</td>\n",
       "      <td>KL</td>\n",
       "      <td>114.000</td>\n",
       "      <td>...</td>\n",
       "      <td>SILVASSA</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>385981.20</td>\n",
       "      <td>200</td>\n",
       "      <td>82500.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1965645.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>9503659139</td>\n",
       "      <td>10</td>\n",
       "      <td>Mar 2024</td>\n",
       "      <td>2024-03-30</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO36209</td>\n",
       "      <td>POWEROIL TO1020 60SNX INHIBITED HG18GE20</td>\n",
       "      <td>SIT0130324</td>\n",
       "      <td>KL</td>\n",
       "      <td>90.000</td>\n",
       "      <td>...</td>\n",
       "      <td>SILVASSA</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490941.00</td>\n",
       "      <td>6000</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2727450.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>9503659140</td>\n",
       "      <td>10</td>\n",
       "      <td>Mar 2024</td>\n",
       "      <td>2024-03-30</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO36209</td>\n",
       "      <td>POWEROIL TO1020 60SNX INHIBITED HG18GE20</td>\n",
       "      <td>SIT0130324</td>\n",
       "      <td>KL</td>\n",
       "      <td>31.000</td>\n",
       "      <td>...</td>\n",
       "      <td>SILVASSA</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169101.90</td>\n",
       "      <td>6000</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>939455.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>9503659142</td>\n",
       "      <td>10</td>\n",
       "      <td>Mar 2024</td>\n",
       "      <td>2024-03-30</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO51209</td>\n",
       "      <td>POWEROIL TO 1020 60 SNU UNINHIBITED 18GE</td>\n",
       "      <td>SIT0080324</td>\n",
       "      <td>KL</td>\n",
       "      <td>113.000</td>\n",
       "      <td>...</td>\n",
       "      <td>SILVASSA</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>455926.19</td>\n",
       "      <td>250</td>\n",
       "      <td>107250.0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2532923.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1602 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Invoice No  Line Item     Month  Order Date Order Type    Material  \\\n",
       "0     9501684612         10  Jan 2024  2024-01-03        ZFC  AE1TO60209   \n",
       "1     9501684613         10  Jan 2024  2024-01-03        ZFC  ABUTO60000   \n",
       "2     9501684614         10  Jan 2024  2024-01-03        ZFC  AE1TO60209   \n",
       "3     9501684619         10  Jan 2024  2024-01-04        ZFC  ABUTO60000   \n",
       "4     9501684620         10  Jan 2024  2024-01-04        ZFC  AE1TO60209   \n",
       "...          ...        ...       ...         ...        ...         ...   \n",
       "1597  9503659135         10  Mar 2024  2024-03-30        ZFC  ABUTO60000   \n",
       "1598  9503659137         10  Mar 2024  2024-03-30        ZFC  AE1TO60209   \n",
       "1599  9503659139         10  Mar 2024  2024-03-30        ZFC  AE1TO36209   \n",
       "1600  9503659140         10  Mar 2024  2024-03-30        ZFC  AE1TO36209   \n",
       "1601  9503659142         10  Mar 2024  2024-03-30        ZFC  AE1TO51209   \n",
       "\n",
       "                                 Mat. Grp Desc    Batch No Base UoM  \\\n",
       "0     POWEROIL TO 335H UNINHI TYPE II 18GE209L  RIT0002A24       KL   \n",
       "1      POWEROIL TO 335H UNINHIBIT TYPE II BULK  RIT0002A24       KL   \n",
       "2     POWEROIL TO 335H UNINHI TYPE II 18GE209L  RIT0002A24       KL   \n",
       "3      POWEROIL TO 335H UNINHIBIT TYPE II BULK  RIT0002A24       KL   \n",
       "4     POWEROIL TO 335H UNINHI TYPE II 18GE209L  RIT0539L23       KL   \n",
       "...                                        ...         ...      ...   \n",
       "1597   POWEROIL TO 335H UNINHIBIT TYPE II BULK  SIT0020324       KL   \n",
       "1598  POWEROIL TO 335H UNINHI TYPE II 18GE209L  SIT0020324       KL   \n",
       "1599  POWEROIL TO1020 60SNX INHIBITED HG18GE20  SIT0130324       KL   \n",
       "1600  POWEROIL TO1020 60SNX INHIBITED HG18GE20  SIT0130324       KL   \n",
       "1601  POWEROIL TO 1020 60 SNU UNINHIBITED 18GE  SIT0080324       KL   \n",
       "\n",
       "      Qty in Sales Unit  ...  Plant Desc       CGST       SGST  UGST  \\\n",
       "0                85.000  ...  RABALE DOM  131105.70  131105.70   0.0   \n",
       "1                11.973  ...  RABALE DOM   78123.83   78123.83   0.0   \n",
       "2                85.000  ...  RABALE DOM  131105.70  131105.70   0.0   \n",
       "3                24.952  ...  RABALE DOM  159443.28  159443.28   0.0   \n",
       "4                32.000  ...  RABALE DOM   52367.04   52367.04   0.0   \n",
       "...                 ...  ...         ...        ...        ...   ...   \n",
       "1597             25.006  ...    SILVASSA       0.00       0.00   0.0   \n",
       "1598            114.000  ...    SILVASSA       0.00       0.00   0.0   \n",
       "1599             90.000  ...    SILVASSA       0.00       0.00   0.0   \n",
       "1600             31.000  ...    SILVASSA       0.00       0.00   0.0   \n",
       "1601            113.000  ...    SILVASSA       0.00       0.00   0.0   \n",
       "\n",
       "           IGST  Commission_N  Basic Sale Price  Credit Days  Discount  \\\n",
       "0          0.00             0           82000.0            0         0   \n",
       "1          0.00          1000           72500.0           60         0   \n",
       "2          0.00             0           82000.0            0         0   \n",
       "3          0.00             0           71000.0           90         0   \n",
       "4          0.00             0           87000.0          180         0   \n",
       "...         ...           ...               ...          ...       ...   \n",
       "1597  339831.54           100           75500.0          100         0   \n",
       "1598  385981.20           200           82500.0           70         0   \n",
       "1599  490941.00          6000          145000.0           17         0   \n",
       "1600  169101.90          6000          145000.0           17         0   \n",
       "1601  455926.19           250          107250.0          150         0   \n",
       "\n",
       "         Revenue  \n",
       "0     1456730.00  \n",
       "1      868042.50  \n",
       "2     1456730.00  \n",
       "3     1771592.00  \n",
       "4      581856.00  \n",
       "...          ...  \n",
       "1597  1887953.00  \n",
       "1598  1965645.00  \n",
       "1599  2727450.00  \n",
       "1600   939455.00  \n",
       "1601  2532923.25  \n",
       "\n",
       "[1602 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "279432df-d08e-4865-bf44-c255fca4cd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Invoice No', 'Line Item', 'Month', 'Order Date', 'Order Type',\n",
       "       'Material', 'Mat. Grp Desc', 'Batch No', 'Base UoM',\n",
       "       'Qty in Sales Unit', 'Sales Unit', 'Sales Qty', 'Frieght Charged',\n",
       "       'Freight Incurred', 'Interest', 'Net Value /KL (NET_VAL_KL)',\n",
       "       'Packing Cost /KL', 'Total Packing Cost', 'RM Cost /KL',\n",
       "       'Total RM Cost', 'Total Variable Cost', 'Total Value', 'NET_CONT_KL',\n",
       "       'DENSITY', 'Sales District', 'Region', 'REG_DESC', 'Payer', 'Customer',\n",
       "       'Employee', 'Sales manager', 'Industry Code', 'Customer Grp',\n",
       "       'Industry Desc', 'MATKL', 'Mat Grp1 Text', 'Mat Grp 2 Text', 'City',\n",
       "       'Doc Curr', 'Plant', 'Plant Desc', 'CGST', 'SGST', 'UGST', 'IGST',\n",
       "       'Commission_N', 'Basic Sale Price', 'Credit Days', 'Discount',\n",
       "       'Revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.get_data().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1bec442-4346-41f1-8f94-74ac5726fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataFramePreprocessor(columns_to_drop=['Invoice No', 'Line Item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c180a4f-c848-46da-b8b3-3b7dcf8e84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = processor.fit_transform(ingestor.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f175510-4d08-4de0-8094-1ea7baa2683d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Qty in Sales Unit', 'Sales Qty', 'Frieght Charged', 'Freight Incurred',\n",
       "       'Interest', 'Net Value /KL (NET_VAL_KL)', 'Packing Cost /KL',\n",
       "       'Total Packing Cost', 'RM Cost /KL', 'Total RM Cost',\n",
       "       'Total Variable Cost', 'Total Value', 'NET_CONT_KL', 'DENSITY',\n",
       "       'Region', 'Plant', 'CGST', 'SGST', 'UGST', 'IGST', 'Commission_N',\n",
       "       'Basic Sale Price', 'Credit Days', 'Discount', 'Revenue', 'Month',\n",
       "       'Order Date', 'Order Type', 'Material', 'Mat. Grp Desc', 'Batch No',\n",
       "       'Base UoM', 'Sales Unit', 'Sales District', 'REG_DESC', 'Payer',\n",
       "       'Customer', 'Employee', 'Sales manager', 'Industry Code',\n",
       "       'Customer Grp', 'Industry Desc', 'MATKL', 'Mat Grp1 Text',\n",
       "       'Mat Grp 2 Text', 'City', 'Doc Curr', 'Plant Desc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ee4daf-2c93-47b8-a09f-0e57a079201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Qty in Sales Unit', 'Sales Qty',\n",
    "        'Sales Unit', 'Sales District',\n",
    "       'REG_DESC', 'Payer', 'Customer', 'Employee', 'Sales manager',\n",
    "       'Industry Code', 'Customer Grp', 'Industry Desc', 'MATKL',\n",
    "       'Mat Grp1 Text', 'Mat Grp 2 Text', 'City', 'Doc Curr', 'Plant Desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47720650-25e2-4c8d-bca0-619a0dcf7209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Qty in Sales Unit', 'Sales Qty', 'Frieght Charged', 'Freight Incurred',\n",
       "       'Interest', 'Net Value /KL (NET_VAL_KL)', 'Packing Cost /KL',\n",
       "       'Total Packing Cost', 'RM Cost /KL', 'Total RM Cost',\n",
       "       'Total Variable Cost', 'Total Value', 'NET_CONT_KL', 'DENSITY',\n",
       "       'Region', 'Plant', 'CGST', 'SGST', 'UGST', 'IGST', 'Commission_N',\n",
       "       'Basic Sale Price', 'Credit Days', 'Discount', 'Revenue', 'Month',\n",
       "       'Order Date', 'Order Type', 'Material', 'Mat. Grp Desc', 'Batch No',\n",
       "       'Base UoM', 'Sales Unit', 'Sales District', 'REG_DESC', 'Payer',\n",
       "       'Customer', 'Employee', 'Sales manager', 'Industry Code',\n",
       "       'Customer Grp', 'Industry Desc', 'MATKL', 'Mat Grp1 Text',\n",
       "       'Mat Grp 2 Text', 'City', 'Doc Curr', 'Plant Desc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79e045a8-08f0-46c9-aec3-b480b168e513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building cluster tree for KPI: Revenue\n",
      "Selecting features for target: Revenue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiva/Desktop/Instinct_AI/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 20 features for Revenue\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ClusterNode.__init__() got multiple values for argument 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_cluster_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns_to_analyze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkpi_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRevenue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRM Cost /KL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Instinct_AI/src/components/clustering.py:360\u001b[0m, in \u001b[0;36mClusteringEngine.build_cluster_trees\u001b[0;34m(self, df, columns_to_analyze, kpi_columns)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Build tree for this specific KPI\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Use a feature-filtered dataframe for clustering, but keep original for analysis\u001b[39;00m\n\u001b[1;32m    358\u001b[0m df_features \u001b[38;5;241m=\u001b[39m df[features \u001b[38;5;241m+\u001b[39m [kpi_column]]\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m kpi_column \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m df[features]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 360\u001b[0m root_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkpi_column\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperform_analysis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns_to_analyze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_analyze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkpi_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkpi_column\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Store the tree\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkpi_trees[kpi_column] \u001b[38;5;241m=\u001b[39m root_node\n",
      "File \u001b[0;32m~/Desktop/Instinct_AI/src/components/clustering.py:261\u001b[0m, in \u001b[0;36mClusteringEngine._build_tree\u001b[0;34m(self, df_features, indices, level, path_prefix, perform_analysis, columns_to_analyze, kpi_column)\u001b[0m\n\u001b[1;32m    258\u001b[0m sub_df_features \u001b[38;5;241m=\u001b[39m df_features\u001b[38;5;241m.\u001b[39mloc[indices]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Create node with the provided indices (which are the actual DataFrame indices)\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[43mClusterNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_df_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mpath_prefix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkpi_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkpi_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Perform dataset comparison analysis using the ORIGINAL df\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m perform_analysis \u001b[38;5;129;01mand\u001b[39;00m columns_to_analyze \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Use full original dataframe and segment from original dataframe\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ClusterNode.__init__() got multiple values for argument 'path'"
     ]
    }
   ],
   "source": [
    "root = engine.build_cluster_trees(data,columns_to_analyze=cols,kpi_columns=['Revenue','RM Cost /KL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9fac6-ddb7-4c98-b2f9-28268ebbb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root['Revenue'].children[0].analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09f70c-192f-49e3-ab5f-7ca0e5230630",
   "metadata": {},
   "outputs": [],
   "source": [
    "root['Revenue'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57720863-fae5-48e3-ace4-3cc8f17f6acb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "root.[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f707afd-e025-4ec0-92ce-de3733dec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "root['Revenue'].save_to_json('model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e4daf-0b03-4eb7-b4e1-e4349e9890eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.clustering import ClusterNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdff2a-388e-4c74-a548-8d9eb2dc25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dde833-a6e6-4cb9-b03f-72e121862ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.get_data().iloc[engine.get_node_by_level_or_path(root,level=1)[0].indices,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ee505-7615-4e45-ae6a-fc16197d2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.get_node_dataframe(engine.get_node_by_level_or_path(root,level=1)[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de850cca-74ea-4826-89cc-1c233b256a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "root.save_to_json('model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fae315-3223-444f-8116-aa74c4145eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2879d-e279-40b0-8d53-9f74bbbafcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.clustering import ClusterNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2f146-4c5d-4790-a1f1-8ccbddb4f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = ClusterNode.load_from_json('model.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4fdeb-ee12-4c58-b33d-7b8d60ece141",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.get_all_clusters_at_level(node,level=1)['root_root_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b03ae3-c726-4e59-b41d-0be8c81265e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.getFile('6471fac1-2c12-4b31-b4f4-0c8b8a1ae121.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4410eac-fe39-4d47-9942-9d9ca43f5d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Acess-key : AKIA3TD2SNPLQV62TGVT\\n',\n",
       " b'Secret Acess-key : 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\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7df3c6ed-884d-4195-91a8-c821406d02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.getFile('b81395fd-acac-493c-a93a-79cce265a7ba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2619252b-d5ca-4c35-9016-429ec84c86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from boto3 import resource\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import List, Optional\n",
    "import io\n",
    "\n",
    "\n",
    "class S3Client:\n",
    "    def __init__(self, bucket_name: str, access_key:Optional[str] = None,secret_key:Optional[str] = None,region_name: Optional[str] = None):\n",
    "        self.resource = resource(\"s3\", aws_access_key_id=access_key,aws_secret_access_key=secret_key,region_name=region_name)\n",
    "        self.bucket = self.resource.Bucket(bucket_name)\n",
    "        self.bucket_name = bucket_name\n",
    "        if secret_key and access_key is None:\n",
    "            raise ValueError(\"Access key must be provided if secret key is provided.\")\n",
    "        self.__secret_key = secret_key\n",
    "        self.__access_key = access_key\n",
    "        self.s3 = boto3.client(\"s3\", aws_access_key_id = access_key,aws_secret_access_key = secret_key,region_name=region_name)\n",
    "\n",
    "    def create_folder(self, folder_name: str) -> str:\n",
    "        try:\n",
    "            self.s3.put_object(Bucket=self.bucket_name, Key=(folder_name + \"/\"))\n",
    "            return f\"Folder {folder_name} created successfully.\"\n",
    "        except ClientError as e:\n",
    "            raise Exception(f\"Folder creation failed: {str(e)}\")\n",
    "        \n",
    "\n",
    "    def upload_file(self, file_stream,folder_name,filename: str) -> str:\n",
    "        try:\n",
    "            root_folder = \"projects/\"\n",
    "            if folder_name and not folder_name.endswith(\"/\"):\n",
    "                folder_name += \"/\"\n",
    "                folder_name = root_folder + folder_name\n",
    "                self.s3.upload_fileobj(file_stream, self.bucket_name, folder_name + filename)\n",
    "            else:\n",
    "                self.s3.upload_fileobj(file_stream, self.bucket_name, filename)\n",
    "            return f\"Uploaded {filename} successfully.\"\n",
    "        except ClientError as e:\n",
    "            raise Exception(f\"Upload failed: {str(e)}\")\n",
    "\n",
    "    def download_file(self, filename: str) -> bytes:\n",
    "        try:\n",
    "            s3_object = self.s3.get_object(Bucket=self.bucket_name, Key=filename)\n",
    "            return s3_object[\"Body\"].read()\n",
    "        except ClientError as e:\n",
    "            raise Exception(f\"Download failed: {str(e)}\")\n",
    "        \n",
    "    def getFile(self, project_id:str) -> io.BytesIO:\n",
    "        try:\n",
    "            folder = \"projects/\"\n",
    "            # file_path = folder + project_id + \"/\"\n",
    "            s3_object = self.s3.get_object(Bucket=self.bucket_name, Key=folder + project_id + \"/raw_data.parquet\")\n",
    "            return io.BytesIO(s3_object[\"Body\"].read())\n",
    "        except ClientError as e:\n",
    "            raise Exception(f\"Download failed: {str(e)}\")\n",
    "\n",
    "    def list_projects(self) -> List[str]:\n",
    "        try:\n",
    "            response = self.bucket.objects.all()\n",
    "            return [str(content.key).split('/')[1] for content in response]\n",
    "        except ClientError as e:\n",
    "            raise Exception(f\"List failed: {str(e)}\")\n",
    "\n",
    "\n",
    "    def upload_dataframe(self,df:pd.DataFrame,project_id:str):\n",
    "        buffer = io.BytesIO()\n",
    "        file_name = \"raw_data.parquet\"\n",
    "        df.to_parquet(buffer)\n",
    "        buffer.seek(0)\n",
    "        self.upload_file(buffer,folder_name=project_id,filename = file_name)\n",
    "        \n",
    "    \n",
    "    # def get_dataframe(self,project_id:str):\n",
    "    #     df = pd.read_parquet(self.getFile(project_id))\n",
    "\n",
    "    def update_file(self, file_stream, filename: str) -> str:\n",
    "        # S3 doesn't distinguish between create and update – both use upload\n",
    "        return self.upload_file(file_stream, filename)\n",
    "\n",
    "    def delete_pro(self, filename: str) -> str:\n",
    "        try:\n",
    "            self.s3.delete_object(Bucket=self.bucket_name, Key=filename)\n",
    "            return f\"Deleted {filename} successfully.\"\n",
    "        except ClientError as e:\n",
    "            raise Exception(f\"Delete failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9212e0-1c67-4243-98c0-c96106a9a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "from src.s3 import S3Client\n",
    "access_key = os.getenv(\"ACCESS_KEY\")\n",
    "secret_key = os.getenv(\"SECRET_KEY\")\n",
    "client = S3Client(\"instinct-ai\", access_key=access_key,secret_key=secret_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b62c7af-867d-4685-a265-73438004d608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae8b4308-7454-4702-bf04-19cdc67d7812\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "df = pd.read_csv('data/cleaned_apar.csv')\n",
    "project_id = str(uuid.uuid4())\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "944be97d-b3e3-49a7-b087-2fc342d195d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_dataframe(df,project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6968bf3c-feb3-4f89-9a65-4e556447b7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0f47a94a-a3c8-4aa1-b709-1f28ed5b2e70',\n",
       " '101',\n",
       " '1011',\n",
       " '102',\n",
       " '1111',\n",
       " '123',\n",
       " '202',\n",
       " '2c3d33e5-3b91-4721-9bc5-2d23b842b6c1',\n",
       " '3683b57e-43d5-4e75-b871-37b92ca25c71',\n",
       " '6a45b475-7195-4f04-99f2-8ebb94b91226',\n",
       " '84151dc5-e03d-44f8-8747-5b2f746ad656',\n",
       " '8be76916-6184-409e-af72-5e471e26faa5',\n",
       " 'a9c29591-7adc-4315-af74-8087c193b92e',\n",
       " 'ae8b4308-7454-4702-bf04-19cdc67d7812',\n",
       " 'b0a885cb-9a1b-4c7d-b11d-7cc4389811c2',\n",
       " 'bbf0eea3-b0c3-4415-8ed8-84a396348ca0',\n",
       " 'c9f13d98-2efd-4fb6-b82d-2a08ed0ae678',\n",
       " 'dfb026e5-fefd-4de2-8c65-64d67a5298a9',\n",
       " 'test']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b747d5d-aa14-45a5-a8b3-c44531c16b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BytesIO at 0x77a635bb3f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.getFile('2c3d33e5-3b91-4721-9bc5-2d23b842b6c1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb48b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6df8aa-0dc5-420b-abd1-e37344294b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice No</th>\n",
       "      <th>Line Item</th>\n",
       "      <th>Month</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Order Type</th>\n",
       "      <th>Material</th>\n",
       "      <th>Mat. Grp Desc</th>\n",
       "      <th>Batch No</th>\n",
       "      <th>Base UoM</th>\n",
       "      <th>Qty in Sales Unit</th>\n",
       "      <th>...</th>\n",
       "      <th>Plant Desc</th>\n",
       "      <th>CGST</th>\n",
       "      <th>SGST</th>\n",
       "      <th>UGST</th>\n",
       "      <th>IGST</th>\n",
       "      <th>Commission_N</th>\n",
       "      <th>Basic Sale Price</th>\n",
       "      <th>Credit Days</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9501684612</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO60209</td>\n",
       "      <td>POWEROIL TO 335H UNINHI TYPE II 18GE209L</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>85.000</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1456730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9501684613</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>ABUTO60000</td>\n",
       "      <td>POWEROIL TO 335H UNINHIBIT TYPE II BULK</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>11.973</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>78123.83</td>\n",
       "      <td>78123.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>72500.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>868042.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9501684614</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO60209</td>\n",
       "      <td>POWEROIL TO 335H UNINHI TYPE II 18GE209L</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>85.000</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>131105.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1456730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9501684619</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>ABUTO60000</td>\n",
       "      <td>POWEROIL TO 335H UNINHIBIT TYPE II BULK</td>\n",
       "      <td>RIT0002A24</td>\n",
       "      <td>KL</td>\n",
       "      <td>24.952</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>159443.28</td>\n",
       "      <td>159443.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1771592.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9501684620</td>\n",
       "      <td>10</td>\n",
       "      <td>Jan 2024</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>ZFC</td>\n",
       "      <td>AE1TO60209</td>\n",
       "      <td>POWEROIL TO 335H UNINHI TYPE II 18GE209L</td>\n",
       "      <td>RIT0539L23</td>\n",
       "      <td>KL</td>\n",
       "      <td>32.000</td>\n",
       "      <td>...</td>\n",
       "      <td>RABALE DOM</td>\n",
       "      <td>52367.04</td>\n",
       "      <td>52367.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>87000.0</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>581856.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Invoice No  Line Item     Month  Order Date Order Type    Material  \\\n",
       "0  9501684612         10  Jan 2024  2024-01-03        ZFC  AE1TO60209   \n",
       "1  9501684613         10  Jan 2024  2024-01-03        ZFC  ABUTO60000   \n",
       "2  9501684614         10  Jan 2024  2024-01-03        ZFC  AE1TO60209   \n",
       "3  9501684619         10  Jan 2024  2024-01-04        ZFC  ABUTO60000   \n",
       "4  9501684620         10  Jan 2024  2024-01-04        ZFC  AE1TO60209   \n",
       "\n",
       "                              Mat. Grp Desc    Batch No Base UoM  \\\n",
       "0  POWEROIL TO 335H UNINHI TYPE II 18GE209L  RIT0002A24       KL   \n",
       "1   POWEROIL TO 335H UNINHIBIT TYPE II BULK  RIT0002A24       KL   \n",
       "2  POWEROIL TO 335H UNINHI TYPE II 18GE209L  RIT0002A24       KL   \n",
       "3   POWEROIL TO 335H UNINHIBIT TYPE II BULK  RIT0002A24       KL   \n",
       "4  POWEROIL TO 335H UNINHI TYPE II 18GE209L  RIT0539L23       KL   \n",
       "\n",
       "   Qty in Sales Unit  ...  Plant Desc       CGST       SGST  UGST  IGST  \\\n",
       "0             85.000  ...  RABALE DOM  131105.70  131105.70   0.0   0.0   \n",
       "1             11.973  ...  RABALE DOM   78123.83   78123.83   0.0   0.0   \n",
       "2             85.000  ...  RABALE DOM  131105.70  131105.70   0.0   0.0   \n",
       "3             24.952  ...  RABALE DOM  159443.28  159443.28   0.0   0.0   \n",
       "4             32.000  ...  RABALE DOM   52367.04   52367.04   0.0   0.0   \n",
       "\n",
       "   Commission_N  Basic Sale Price  Credit Days  Discount    Revenue  \n",
       "0             0           82000.0            0         0  1456730.0  \n",
       "1          1000           72500.0           60         0   868042.5  \n",
       "2             0           82000.0            0         0  1456730.0  \n",
       "3             0           71000.0           90         0  1771592.0  \n",
       "4             0           87000.0          180         0   581856.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46f7b7d0-00aa-4a50-a69f-ac6f957ddf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = client.get_dataframe('ae8b4308-7454-4702-bf04-19cdc67d7812')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebdc58af-dec7-4ed6-b0a4-10647e8275d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19067a95-dde7-45d5-8630-1617efafe1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Union, Optional, Tuple\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.metrics import silhouette_score\n",
    "import shap\n",
    "from auto_shap.auto_shap import generate_shap_values\n",
    "\n",
    "class ClusterNode:\n",
    "    def __init__(self, level, data_indices, df, path):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.level = level\n",
    "        # Store the actual indices from the original dataframe\n",
    "        self.indices = data_indices\n",
    "        # Calculate centroid using the actual rows\n",
    "        # self.centroid = df.loc[data_indices].mean(axis=0).tolist()\n",
    "        self.size = len(data_indices)\n",
    "        self.path = path\n",
    "        self.children = []\n",
    "        self.score = None\n",
    "        self.analysis = None\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"level\": self.level,\n",
    "            # \"centroid\": self.centroid,\n",
    "            \"size\": self.size,\n",
    "            \"indices\": self.indices,\n",
    "            \"path\": self.path,\n",
    "            \"score\": self.score,\n",
    "            \"analysis\": self.analysis,\n",
    "            \"children\": [child.to_dict() for child in self.children]\n",
    "        }\n",
    "\n",
    "\n",
    "class ClusteringEngine:\n",
    "    def __init__(self, max_depth=3, min_cluster_size=10, max_k=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.max_k = max_k\n",
    "        self.original_df = None\n",
    "        self.raw_df = None\n",
    "        self.selected_features = None\n",
    "\n",
    "    @staticmethod\n",
    "    def feature_selector(\n",
    "        data_frame: pd.DataFrame,\n",
    "    target_columns: Union[str, List[str]],\n",
    "    n_features: Optional[int] = None,\n",
    "    threshold: Optional[float] = None,\n",
    "    user_selected_features: Optional[List[str]] = None,\n",
    "    plot_importance: bool = False,\n",
    "    target_weights: Optional[Dict[str, float]] = None,\n",
    "    random_state: int = 42\n",
    ") -> List[str]:\n",
    "        \"\"\"\n",
    "    Select the most important features using SHAP values, supporting multiple target columns\n",
    "    and user-specified feature preferences. Task type (regression/classification) is automatically\n",
    "    determined based on the data type of the target column(s).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_frame : pandas DataFrame\n",
    "        DataFrame containing both features and target column(s)\n",
    "    target_columns : str or list of str\n",
    "        Name(s) of the target column(s) in the DataFrame\n",
    "    n_features : int, optional\n",
    "        Number of top features to select. Either n_features or threshold must be provided.\n",
    "    threshold : float, optional\n",
    "        Threshold for cumulative importance. Features are selected until their\n",
    "        cumulative importance exceeds this threshold (0-1). Either n_features or threshold must be provided.\n",
    "    user_selected_features : list of str, optional\n",
    "        List of feature names that the user believes are important and should be included\n",
    "        regardless of their SHAP importance\n",
    "    plot_importance : bool, default=True\n",
    "        Whether to plot feature importance\n",
    "    target_weights : dict, optional\n",
    "        Dictionary mapping target column names to their importance weights.\n",
    "        If not provided, all targets are weighted equally.\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_features : list\n",
    "        List of selected feature names\n",
    "    \"\"\"\n",
    "        # Convert target_columns to list if it's a single string\n",
    "        if isinstance(target_columns, str):\n",
    "            target_columns = [target_columns]\n",
    "        \n",
    "        # Input validation\n",
    "        for col in target_columns:\n",
    "            if col not in data_frame.columns:\n",
    "                raise ValueError(f\"Target column '{col}' does not exist in the data frame\")\n",
    "        \n",
    "        if n_features is None and threshold is None:\n",
    "            raise ValueError(\"Either n_features or threshold must be provided\")\n",
    "        \n",
    "        if threshold is not None and (threshold <= 0 or threshold > 1):\n",
    "            raise ValueError(\"Threshold must be between 0 and 1\")\n",
    "        \n",
    "        if n_features is not None and n_features <= 0:\n",
    "            raise ValueError(\"n_features must be positive\")\n",
    "        \n",
    "        # Separate features and targets\n",
    "        X = data_frame.drop(columns=target_columns)\n",
    "        \n",
    "        if n_features is not None and n_features > X.shape[1]:\n",
    "            n_features = X.shape[1]\n",
    "            print(f\"Warning: n_features was greater than the number of available features. Setting to {n_features}\")\n",
    "        \n",
    "        # Validate user_selected_features\n",
    "        if user_selected_features is not None:\n",
    "            # Convert to set for faster lookup\n",
    "            user_features_set = set(user_selected_features)\n",
    "            invalid_features = user_features_set - set(X.columns)\n",
    "            if invalid_features:\n",
    "                raise ValueError(f\"The following user-selected features are not in the dataset: {invalid_features}\")\n",
    "            \n",
    "            # Check if user selected all features\n",
    "            if len(user_features_set) == X.shape[1]:\n",
    "                print(\"User selected all features. No SHAP-based selection will be performed.\")\n",
    "                return list(X.columns)\n",
    "        else:\n",
    "            user_features_set = set()\n",
    "        \n",
    "        # Determine task type for each target column\n",
    "        task_types = {}\n",
    "        for col in target_columns:\n",
    "            # Check if the column contains categorical data\n",
    "            unique_values = data_frame[col].nunique()\n",
    "            if pd.api.types.is_numeric_dtype(data_frame[col]):\n",
    "                # For numeric types, if there are few unique values and they're all integers, it's likely classification\n",
    "                if unique_values <= 10 and np.array_equal(data_frame[col], data_frame[col].astype(int)):\n",
    "                    task_types[col] = 'classification'\n",
    "                else:\n",
    "                    task_types[col] = 'regression'\n",
    "            else:\n",
    "                # Non-numeric types are always classification\n",
    "                task_types[col] = 'classification'\n",
    "        \n",
    "        # Print determined task types\n",
    "        print(\"Automatically determined task types:\")\n",
    "        for col, task_type in task_types.items():\n",
    "            print(f\"- {col}: {task_type}\")\n",
    "        \n",
    "        # Handle target weights\n",
    "        if target_weights is None:\n",
    "            # Equal weights for all targets\n",
    "            target_weights = {name: 1.0/len(target_columns) for name in target_columns}\n",
    "        else:\n",
    "            # Validate target weights\n",
    "            missing_targets = set(target_columns) - set(target_weights.keys())\n",
    "            if missing_targets:\n",
    "                warnings.warn(f\"Target weights not provided for: {missing_targets}. Using default weight of 0.\")\n",
    "                for target in missing_targets:\n",
    "                    target_weights[target] = 0.0\n",
    "                    \n",
    "            # Normalize weights to sum to 1\n",
    "            weight_sum = sum(target_weights.values())\n",
    "            if weight_sum == 0:\n",
    "                raise ValueError(\"Sum of target weights cannot be zero\")\n",
    "            target_weights = {k: v/weight_sum for k, v in target_weights.items()}\n",
    "        \n",
    "        print(f\"Processing {len(target_columns)} target{'s' if len(target_columns) > 1 else ''}\")\n",
    "        for target, weight in target_weights.items():\n",
    "            print(f\"- {target}: weight = {weight:.3f}\")\n",
    "        \n",
    "        # Calculate feature importance for each target\n",
    "        feature_importance_dict = {}\n",
    "        shap_values_dict = {}\n",
    "        \n",
    "        for target_name in target_columns:\n",
    "            if target_weights.get(target_name, 0) == 0:\n",
    "                print(f\"Skipping target '{target_name}' with weight 0\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Computing feature importance for target: {target_name}\")\n",
    "            \n",
    "            # Get target values\n",
    "            y = data_frame[target_name]\n",
    "            \n",
    "            # Get task type for this target\n",
    "            task_type = task_types[target_name]\n",
    "            \n",
    "            # Initialize model based on task type\n",
    "            if task_type == 'regression':\n",
    "                model = RandomForestRegressor(n_estimators=100, n_jobs = -1, random_state=random_state)\n",
    "            elif task_type == 'classification':\n",
    "                model = RandomForestClassifier(n_estimators=100,n_jobs = -1, random_state=random_state)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid task type '{task_type}' for target '{target_name}'\")\n",
    "            \n",
    "            # Fit the model\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            # Create explainer and compute SHAP values\n",
    "            _,_,shap_values = generate_shap_values(model,X)\n",
    "            # if len(shap_values.shape) == 3:\n",
    "            #     shap_values = shap_values[:,:,-1]\n",
    "            # shap_values = shap_values[:,:,-1]\n",
    "            \n",
    "            # For classification tasks with multiple classes, shap_values will be a list\n",
    "            if isinstance(shap_values, list):\n",
    "                # Sum across all classes\n",
    "                shap_values = np.abs(np.array(shap_values)).sum(axis=0)\n",
    "            \n",
    "            # Store for later use\n",
    "            shap_values_dict[target_name] = shap_values['shap_value'].values\n",
    "            \n",
    "            # Calculate feature importance as mean absolute SHAP value for each feature\n",
    "            feature_importance_dict = shap_values.set_index('feature').to_dict()['shap_value']\n",
    "            # feature_importance_dict[target_name] = feature_importance.set_index('feature').to_dict()['shap_value']\n",
    "            \n",
    "        # Combine feature importance from all targets using weights\n",
    "        combined_importance = np.zeros(X.shape[1])\n",
    "        # print(feature_importance_dict)\n",
    "        for target_name, importance in feature_importance_dict.items():\n",
    "            weight = target_weights.get(target_name, 0)\n",
    "            combined_importance += importance * weight\n",
    "        # print(combined_importance)\n",
    "        \n",
    "        # Create DataFrame with feature names and importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': combined_importance,\n",
    "            'User_Selected': [feature in user_features_set for feature in X.columns]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Also create target-specific importance dataframes for plotting\n",
    "        target_importance_dfs = {}\n",
    "        for target_name, importance in feature_importance_dict.items():\n",
    "            target_importance_dfs[target_name] = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Importance': importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Start with user-selected features\n",
    "        selected_features = list(user_features_set)\n",
    "        \n",
    "        # Add features based on SHAP importance (excluding those already selected by user)\n",
    "        remaining_features = feature_importance_df[~feature_importance_df['User_Selected']]\n",
    "        \n",
    "        # Calculate how many additional features to select (if using n_features)\n",
    "        if n_features is not None:\n",
    "            remaining_n = max(0, n_features - len(selected_features))\n",
    "            additional_features = remaining_features['Feature'].tolist()[:remaining_n]\n",
    "            selected_features.extend(additional_features)\n",
    "        else:\n",
    "            # For threshold-based selection, we need to recalculate importance considering already selected features\n",
    "            if selected_features:\n",
    "                # Calculate the importance of already selected features\n",
    "                selected_importance = feature_importance_df[feature_importance_df['User_Selected']]['Importance'].sum()\n",
    "                total_importance = feature_importance_df['Importance'].sum()\n",
    "                \n",
    "                # Calculate the remaining importance needed\n",
    "                remaining_importance_needed = min(threshold * total_importance - selected_importance, \n",
    "                                                remaining_features['Importance'].sum())\n",
    "                \n",
    "                if remaining_importance_needed > 0:\n",
    "                    # Calculate cumulative importance for remaining features\n",
    "                    remaining_features['Cumulative_Importance'] = remaining_features['Importance'].cumsum()\n",
    "                    \n",
    "                    # Select features until we reach the needed importance\n",
    "                    additional_features = remaining_features[remaining_features['Cumulative_Importance'] <= remaining_importance_needed]['Feature'].tolist()\n",
    "                    selected_features.extend(additional_features)\n",
    "            else:\n",
    "                # If no user-selected features, proceed with normal threshold selection\n",
    "                remaining_features['Cumulative_Importance'] = remaining_features['Importance'].cumsum() / feature_importance_df['Importance'].sum()\n",
    "                additional_features = remaining_features[remaining_features['Cumulative_Importance'] <= threshold]['Feature'].tolist()\n",
    "                selected_features.extend(additional_features)\n",
    "        \n",
    "        # If no features were selected, take at least the most important one\n",
    "        if not selected_features:\n",
    "            selected_features = [feature_importance_df['Feature'].iloc[0]]\n",
    "        \n",
    "        print(f\"Selected {len(selected_features)}/{X.shape[1]} features\")\n",
    "        print(f\"- User-selected features: {len(user_features_set)}\")\n",
    "        print(f\"- SHAP-selected features: {len(selected_features) - len(user_features_set)}\")\n",
    "        \n",
    "        return selected_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _best_k_by_silhouette(self, df):\n",
    "        best_k = 2\n",
    "        best_score = -1\n",
    "        X = df.to_numpy()\n",
    "\n",
    "        for k in range(2, min(self.max_k + 1, len(df))):\n",
    "            try:\n",
    "                labels = GaussianMixture(n_components=k, covariance_type='full', random_state=0).fit_predict(X)\n",
    "                score = silhouette_score(X, labels)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_k = k\n",
    "            except Exception as e:\n",
    "                print(f\"Error during clustering with k={k}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return best_k\n",
    "\n",
    "    def _build_tree(self, df, indices, level, path_prefix, perform_analysis=True, columns_to_analyze=None):\n",
    "        \"\"\"\n",
    "        Build the cluster tree recursively.\n",
    "        \n",
    "        Args:\n",
    "            df: The full DataFrame\n",
    "            indices: List of indices from the original DataFrame for this cluster\n",
    "            level: Current depth level\n",
    "            path_prefix: Path prefix for this node\n",
    "            perform_analysis: Whether to perform analysis\n",
    "            columns_to_analyze: Columns to analyze\n",
    "            \n",
    "        Returns:\n",
    "            ClusterNode: The created node\n",
    "        \"\"\"\n",
    "        # Create node with the provided indices (which are the actual DataFrame indices)\n",
    "        node = ClusterNode(level, indices, df, path=[path_prefix])\n",
    "\n",
    "        # Perform dataset comparison analysis\n",
    "        if perform_analysis and columns_to_analyze:\n",
    "            full_df = df.copy()\n",
    "            segment_df = df.loc[indices].copy()\n",
    "            node.analysis = self.compare_datasets(\n",
    "                full_df=full_df,\n",
    "                segment_df=segment_df,\n",
    "                columns_to_analyze=columns_to_analyze\n",
    "            )\n",
    "\n",
    "        # Terminal case: reached max depth or min cluster size\n",
    "        if level >= self.max_depth or len(indices) < self.min_cluster_size:\n",
    "            return node\n",
    "\n",
    "        # Get the subset of data for this cluster\n",
    "        sub_df = df.loc[indices]\n",
    "        \n",
    "        # Find optimal number of clusters\n",
    "        k = self._best_k_by_silhouette(sub_df)\n",
    "        \n",
    "        # Perform clustering on the subset\n",
    "        agg_clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "        cluster_labels = agg_clustering.fit_predict(sub_df)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        node.score = silhouette_score(sub_df, cluster_labels)\n",
    "\n",
    "        # Create a mapping from position in sub_df to actual index in original df\n",
    "        position_to_index = {i: idx for i, idx in enumerate(sub_df.index)}\n",
    "\n",
    "        # Process each cluster\n",
    "        for cluster_id in range(k):\n",
    "            # Find positions where the cluster label matches\n",
    "            cluster_positions = np.where(cluster_labels == cluster_id)[0]\n",
    "            \n",
    "            # Map these positions back to actual DataFrame indices\n",
    "            cluster_indices = [position_to_index[pos] for pos in cluster_positions]\n",
    "            \n",
    "            if len(cluster_indices) >= self.min_cluster_size:\n",
    "                child_path = f\"{path_prefix}_{cluster_id}\"\n",
    "                child_node = self._build_tree(\n",
    "                    df, \n",
    "                    cluster_indices, \n",
    "                    level + 1, \n",
    "                    child_path, \n",
    "                    perform_analysis, \n",
    "                    columns_to_analyze\n",
    "                )\n",
    "                child_node.path = node.path + [child_path]\n",
    "                node.children.append(child_node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def build_cluster_tree(self, df, columns_to_analyze=None, kpi_columns=None):\n",
    "        \"\"\"\n",
    "        Build a cluster tree and analyze each segment.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to cluster\n",
    "            columns_to_analyze: List of columns to analyze in each cluster\n",
    "            \n",
    "        Returns:\n",
    "            The root node of the cluster tree\n",
    "        \"\"\"\n",
    "        # Store the original dataframe for later retrieval\n",
    "        self.original_df = df.copy()\n",
    "        \n",
    "        if columns_to_analyze is None:\n",
    "            columns_to_analyze = df.columns.tolist()\n",
    "\n",
    "        self.selected_features = ClusteringEngine.feature_selector(\n",
    "            data_frame=df,\n",
    "            target_columns=kpi_columns,\n",
    "            user_selected_features=columns_to_analyze,\n",
    "            threshold=0.5\n",
    "        )        \n",
    "        # Use all indices from the original DataFrame\n",
    "        indices = df.index.tolist()\n",
    "        \n",
    "        return self._build_tree(\n",
    "            df[self.selected_features if self.selected_features else df.columns], \n",
    "            indices, \n",
    "            level=0, \n",
    "            path_prefix=\"root\", \n",
    "            perform_analysis=True, \n",
    "            columns_to_analyze=columns_to_analyze\n",
    "        )\n",
    "\n",
    "    def refine_cluster(self, df, indices, n_neighbors=50, columns_to_analyze=None):\n",
    "        \"\"\"\n",
    "        Refine a cluster using nearest neighbors.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing the data\n",
    "            indices: Indices of the data points to refine\n",
    "            n_neighbors: Number of neighbors to consider\n",
    "            columns_to_analyze: List of columns to analyze in the refined cluster\n",
    "            \n",
    "        Returns:\n",
    "            The root node of the refined cluster tree\n",
    "        \"\"\"\n",
    "        # Store the original dataframe for later retrieval if not already stored\n",
    "        if self.original_df is None:\n",
    "            self.original_df = df.copy()\n",
    "            \n",
    "        if columns_to_analyze is None:\n",
    "            columns_to_analyze = df.columns.tolist()\n",
    "            \n",
    "        sub_df = df.loc[indices]\n",
    "        X = sub_df.to_numpy()\n",
    "        \n",
    "        # Use at most the number of points we have\n",
    "        real_n_neighbors = min(n_neighbors, len(sub_df))\n",
    "        nbrs = NearestNeighbors(n_neighbors=real_n_neighbors).fit(X)\n",
    "        _, neighbor_indices = nbrs.kneighbors(X)\n",
    "        \n",
    "        # Map neighbor indices (which are positions in X) back to indices in the sub_df\n",
    "        neighbor_positions = list(set(neighbor_indices.flatten()))\n",
    "        \n",
    "        # Map from positions in sub_df to actual indices in original df\n",
    "        position_to_orig_index = {i: idx for i, idx in enumerate(sub_df.index)}\n",
    "        \n",
    "        # Get the actual DataFrame indices for these neighbors\n",
    "        selected_global_indices = [position_to_orig_index[pos] for pos in neighbor_positions]\n",
    "\n",
    "        return self._build_tree(\n",
    "            df, \n",
    "            selected_global_indices, \n",
    "            level=0, \n",
    "            path_prefix=\"refined\", \n",
    "            perform_analysis=True, \n",
    "            columns_to_analyze=columns_to_analyze\n",
    "        )\n",
    "\n",
    "    # Node retrieval methods\n",
    "    def get_node_by_level_or_path(self, root_node: ClusterNode, level: Optional[int] = None, \n",
    "                                path: Optional[List[str]] = None) -> List[ClusterNode]:\n",
    "        \"\"\"\n",
    "        Find cluster nodes by level and/or path.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            level: The level to search for (if None, ignores level constraint)\n",
    "            path: The path to search for (if None, ignores path constraint)\n",
    "            \n",
    "        Returns:\n",
    "            List of matching cluster nodes\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        \n",
    "        def _traverse(node):\n",
    "            # Check if node matches criteria\n",
    "            level_match = level is None or node.level == level\n",
    "            path_match = path is None or self._path_matches(node.path, path)\n",
    "            \n",
    "            if level_match and path_match:\n",
    "                result.append(node)\n",
    "            \n",
    "            # Continue traversing children\n",
    "            for child in node.children:\n",
    "                _traverse(child)\n",
    "        \n",
    "        _traverse(root_node)\n",
    "        return result\n",
    "    \n",
    "    def _path_matches(self, node_path: List[str], search_path: List[str]) -> bool:\n",
    "        \"\"\"Check if a node's path matches or contains the search path.\"\"\"\n",
    "        if len(search_path) > len(node_path):\n",
    "            return False\n",
    "        \n",
    "        # Check if the beginning of node_path matches search_path\n",
    "        for i in range(len(search_path)):\n",
    "            if search_path[i] != node_path[i]:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def get_node_dataframe(self, node: ClusterNode) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the DataFrame corresponding to a specific cluster node.\n",
    "        \n",
    "        Args:\n",
    "            node: The cluster node to extract data for\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the data points in the cluster\n",
    "        \"\"\"\n",
    "        if self.original_df is None:\n",
    "            raise ValueError(\"No original DataFrame is stored. Call build_cluster_tree first.\")\n",
    "        \n",
    "        # Use node.indices which contain actual DataFrame indices\n",
    "        return self.original_df.loc[node.indices].copy()\n",
    "    \n",
    "    def get_dataframes_by_level_or_path(self, root_node: ClusterNode, level: Optional[int] = None, \n",
    "                                      path: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get DataFrames for nodes matching specified level and/or path.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            level: The level to search for (if None, ignores level constraint)\n",
    "            path: The path to search for (if None, ignores path constraint)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping node IDs to their corresponding DataFrames\n",
    "        \"\"\"\n",
    "        matching_nodes = self.get_node_by_level_or_path(root_node, level, path)\n",
    "        \n",
    "        result = {}\n",
    "        for node in matching_nodes:\n",
    "            node_df = self.get_node_dataframe(node)\n",
    "            # Use node path as the key, converted to a string for dictionary key\n",
    "            key = f\"level_{node.level}_{'_'.join(node.path)}\"\n",
    "            result[key] = node_df\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def find_node_by_exact_path(self, root_node: ClusterNode, path: List[str]) -> Optional[ClusterNode]:\n",
    "        \"\"\"\n",
    "        Find a specific node by its exact path.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            path: The exact path to find\n",
    "            \n",
    "        Returns:\n",
    "            The matching node or None if not found\n",
    "        \"\"\"\n",
    "        def _find(node):\n",
    "            if node.path == path:\n",
    "                return node\n",
    "            \n",
    "            for child in node.children:\n",
    "                result = _find(child)\n",
    "                if result:\n",
    "                    return result\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        return _find(root_node)\n",
    "    \n",
    "    def get_dataframe_by_path_string(self, root_node: ClusterNode, path_string: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame for a node specified by a path string like \"root_0_1\".\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            path_string: A string representing the path, with components separated by underscores\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame for the specified node\n",
    "        \"\"\"\n",
    "        path = path_string.split('_')\n",
    "        node = self.find_node_by_exact_path(root_node, path)\n",
    "        \n",
    "        if node:\n",
    "            return self.get_node_dataframe(node)\n",
    "        else:\n",
    "            raise ValueError(f\"No node found with path: {path_string}\")\n",
    "    \n",
    "    def get_labeled_dataframe(self, root_node: ClusterNode, level: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the original DataFrame with cluster labels added.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            level: The specific level to get labels for (if None, includes all levels)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cluster labels as additional columns\n",
    "        \"\"\"\n",
    "        if self.original_df is None:\n",
    "            raise ValueError(\"No original DataFrame is stored. Call build_cluster_tree first.\")\n",
    "        \n",
    "        # Start with a copy of the original dataframe\n",
    "        result_df = self.original_df.copy()\n",
    "        \n",
    "        # Create a mapping of index to path for each level\n",
    "        index_to_path_by_level = {}\n",
    "        \n",
    "        def _collect_paths(node):\n",
    "            if level is None or node.level == level:\n",
    "                node_level = node.level\n",
    "                if node_level not in index_to_path_by_level:\n",
    "                    index_to_path_by_level[node_level] = {}\n",
    "                \n",
    "                path_str = '_'.join(node.path)\n",
    "                for idx in node.indices:\n",
    "                    index_to_path_by_level[node_level][idx] = path_str\n",
    "            \n",
    "            for child in node.children:\n",
    "                _collect_paths(child)\n",
    "        \n",
    "        _collect_paths(root_node)\n",
    "        \n",
    "        # Add columns for each level\n",
    "        for lvl in sorted(index_to_path_by_level.keys()):\n",
    "            col_name = f\"cluster_level_{lvl}\"\n",
    "            result_df[col_name] = pd.Series(index_to_path_by_level[lvl])\n",
    "            \n",
    "        return result_df\n",
    "\n",
    "    # Get all clusters at a specific level as a dictionary of DataFrames\n",
    "    def get_all_clusters_at_level(self, root_node: ClusterNode, level: int) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get all clusters at a specific level as a dictionary of DataFrames.\n",
    "        \n",
    "        Args:\n",
    "            root_node: The root node of the cluster tree\n",
    "            level: The level to get clusters from\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping cluster paths to their DataFrames\n",
    "        \"\"\"\n",
    "        nodes = self.get_node_by_level_or_path(root_node, level=level)\n",
    "        \n",
    "        result = {}\n",
    "        for node in nodes:\n",
    "            path_str = '_'.join(node.path)\n",
    "            result[path_str] = self.get_node_dataframe(node)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # Dataset comparison functions\n",
    "    def compare_datasets(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        columns_to_analyze: List[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compare a segment to the full dataset across specified columns.\n",
    "        \n",
    "        Args:\n",
    "            full_df: The complete dataset\n",
    "            segment_df: A segment/subset of the dataset\n",
    "            columns_to_analyze: Columns to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        if segment_df.empty:\n",
    "            return {\"error\": \"Segmented dataset is empty\"}\n",
    "        \n",
    "        result = {}\n",
    "        \n",
    "        for col in columns_to_analyze:\n",
    "            if col not in full_df.columns:\n",
    "                result[col] = {\"error\": f\"Column '{col}' not found in dataset\"}\n",
    "                continue\n",
    "                \n",
    "            is_numeric = pd.api.types.is_numeric_dtype(full_df[col])\n",
    "            \n",
    "            if is_numeric:\n",
    "                unique_count = len(full_df[col].dropna().unique())\n",
    "                \n",
    "                if unique_count <= self.discrete_numeric_threshold:\n",
    "                    result[col] = self.analyze_discrete_numeric_column(full_df, segment_df, col)\n",
    "                else:\n",
    "                    result[col] = self.analyze_numeric_column(full_df, segment_df, col)\n",
    "            else:\n",
    "                result[col] = self.analyze_categorical_column(full_df, segment_df, col)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def analyze_categorical_column(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        col: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a categorical column, comparing segment to full dataset.\n",
    "        \"\"\"\n",
    "        full_values = set(full_df[col].dropna().unique())\n",
    "        segment_values = set(segment_df[col].dropna().unique())\n",
    "        categories_in_both = full_values.intersection(segment_values)\n",
    "        full_unique_count = len(full_values)\n",
    "        segment_unique_count = len(segment_values)\n",
    "        coverage_percentage = round((len(categories_in_both) / full_unique_count) * 100, 2) if full_unique_count > 0 else 0\n",
    "\n",
    "        segment_counts = segment_df[col].value_counts()\n",
    "        total_segment_count = segment_df[col].dropna().shape[0]  \n",
    "\n",
    "        mode = segment_counts.index[0] if not segment_counts.empty else None\n",
    "        mode_count = segment_counts.iloc[0] if not segment_counts.empty else 0\n",
    "\n",
    "        top_count = min(3, len(segment_counts))\n",
    "        top_categories = []\n",
    "        for i in range(top_count):\n",
    "            if i < len(segment_counts):\n",
    "                category = segment_counts.index[i]\n",
    "                count = segment_counts.iloc[i]\n",
    "                percentage = round((count / total_segment_count) * 100, 2) if total_segment_count > 0 else 0\n",
    "                top_categories.append({\"category\": str(category), \"percentage\": percentage})\n",
    "\n",
    "        bottom_count = min(3, len(segment_counts))\n",
    "        bottom_categories = []\n",
    "        for i in range(1, bottom_count + 1):\n",
    "            if len(segment_counts) >= i:\n",
    "                category = segment_counts.index[-i]\n",
    "                count = segment_counts.iloc[-i]\n",
    "                percentage = round((count / total_segment_count) * 100, 2) if total_segment_count > 0 else 0\n",
    "                bottom_categories.append({\"category\": str(category), \"percentage\": percentage})\n",
    "        \n",
    "        return {\n",
    "            \"unique_categories\": {\n",
    "                \"segment\": segment_unique_count,\n",
    "                \"full\": full_unique_count,\n",
    "                \"coverage_percentage\": coverage_percentage\n",
    "            },\n",
    "            \"mode\": {\n",
    "                \"category\": str(mode) if mode is not None else None,\n",
    "                \"count\": int(mode_count)\n",
    "            },\n",
    "            \"top_categories\": top_categories,\n",
    "            \"bottom_categories\": bottom_categories\n",
    "        }\n",
    "\n",
    "    def analyze_discrete_numeric_column(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        col: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a discrete numeric column, combining categorical and numeric analysis.\n",
    "        \"\"\"\n",
    "        categorical_analysis = self.analyze_categorical_column(full_df, segment_df, col)\n",
    "        \n",
    "        full_mean = full_df[col].mean()\n",
    "        full_sum = full_df[col].sum()\n",
    "        segment_mean = segment_df[col].mean()\n",
    "        segment_sum = segment_df[col].sum()\n",
    "        \n",
    "        sum_contribution = (segment_sum / full_sum * 100) if full_sum != 0 else 0\n",
    "        mean_contribution = (segment_mean / full_mean * 100) if full_mean != 0 else 0\n",
    "        \n",
    "        categorical_analysis[\"numeric_stats\"] = {\n",
    "            \"full_dataset\": {\n",
    "                \"mean\": float(full_mean),\n",
    "                \"sum\": float(full_sum)\n",
    "            },\n",
    "            \"segment\": {\n",
    "                \"mean\": float(segment_mean),\n",
    "                \"sum\": float(segment_sum)\n",
    "            },\n",
    "            \"contributions\": {\n",
    "                \"sum_contribution_percentage\": round(sum_contribution, 2),\n",
    "                \"mean_contribution_percentage\": round(mean_contribution, 2)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        all_values = {}\n",
    "        for value in sorted(full_df[col].dropna().unique()):\n",
    "            full_count = len(full_df[full_df[col] == value])\n",
    "            segment_count = len(segment_df[segment_df[col] == value])\n",
    "            percentage = round((segment_count / full_count * 100), 2) if full_count > 0 else 0\n",
    "            \n",
    "            if percentage > 50:\n",
    "                all_values[str(value)] = {\n",
    "                    \"full_count\": full_count,\n",
    "                    \"segment_count\": segment_count,\n",
    "                    \"percentage\": percentage\n",
    "                }\n",
    "        \n",
    "        categorical_analysis[\"value_distribution\"] = all_values\n",
    "        \n",
    "        return categorical_analysis\n",
    "\n",
    "    def analyze_numeric_column(\n",
    "        self, \n",
    "        full_df: pd.DataFrame, \n",
    "        segment_df: pd.DataFrame, \n",
    "        col: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a continuous numeric column.\n",
    "        \"\"\"\n",
    "        full_mean = full_df[col].mean()\n",
    "        full_sum = full_df[col].sum()\n",
    "        \n",
    "        segment_mean = segment_df[col].mean()\n",
    "        segment_sum = segment_df[col].sum()\n",
    "        \n",
    "        sum_contribution = (segment_sum / full_sum * 100) if full_sum != 0 else 0\n",
    "        mean_contribution = (segment_mean / full_mean * 100) if full_mean != 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"full_dataset\": {\n",
    "                \"mean\": float(full_mean),\n",
    "                \"sum\": float(full_sum)\n",
    "            },\n",
    "            \"segment\": {\n",
    "                \"mean\": float(segment_mean),\n",
    "                \"sum\": float(segment_sum)\n",
    "            },\n",
    "            \"contributions\": {\n",
    "                \"sum_contribution_percentage\": round(sum_contribution, 2),\n",
    "                \"mean_contribution_percentage\": round(mean_contribution, 2)\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5085b054-6db0-4cdb-bff9-b1c08db3c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.dataingestion import DataIngestion\n",
    "from src.components.datapreprocessing import DataFramePreprocessor\n",
    "ingestor = DataIngestion()\n",
    "processor = DataFramePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4dcda6-acb8-4cc5-905f-18de9f0bf01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice No</th>\n",
       "      <th>Line Item</th>\n",
       "      <th>Qty in Sales Unit</th>\n",
       "      <th>Sales Qty</th>\n",
       "      <th>Frieght Charged</th>\n",
       "      <th>Freight Incurred</th>\n",
       "      <th>Interest</th>\n",
       "      <th>Net Value /KL (NET_VAL_KL)</th>\n",
       "      <th>Packing Cost /KL</th>\n",
       "      <th>Total Packing Cost</th>\n",
       "      <th>...</th>\n",
       "      <th>Sales manager</th>\n",
       "      <th>Industry Code</th>\n",
       "      <th>Customer Grp</th>\n",
       "      <th>Industry Desc</th>\n",
       "      <th>MATKL</th>\n",
       "      <th>Mat Grp1 Text</th>\n",
       "      <th>Mat Grp 2 Text</th>\n",
       "      <th>City</th>\n",
       "      <th>Doc Curr</th>\n",
       "      <th>Plant Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.323463</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>1.390033</td>\n",
       "      <td>0.172374</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>-0.940121</td>\n",
       "      <td>-1.215172</td>\n",
       "      <td>-0.555974</td>\n",
       "      <td>0.814349</td>\n",
       "      <td>1.488775</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.323462</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>-0.842688</td>\n",
       "      <td>-0.380625</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>-0.544604</td>\n",
       "      <td>-0.595787</td>\n",
       "      <td>-0.698871</td>\n",
       "      <td>-1.148312</td>\n",
       "      <td>-0.738807</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.323461</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>1.390033</td>\n",
       "      <td>0.172374</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>-0.940121</td>\n",
       "      <td>-1.215172</td>\n",
       "      <td>-0.555974</td>\n",
       "      <td>0.814349</td>\n",
       "      <td>1.488775</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.323455</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>-0.445869</td>\n",
       "      <td>0.858562</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>-0.219512</td>\n",
       "      <td>-0.305319</td>\n",
       "      <td>-0.769007</td>\n",
       "      <td>-1.148312</td>\n",
       "      <td>-0.738807</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.323454</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>-0.230384</td>\n",
       "      <td>-0.885217</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>-0.683575</td>\n",
       "      <td>1.014611</td>\n",
       "      <td>-0.293881</td>\n",
       "      <td>0.826556</td>\n",
       "      <td>0.105028</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>0.757371</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>-0.444218</td>\n",
       "      <td>0.863718</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>1.604958</td>\n",
       "      <td>-0.140149</td>\n",
       "      <td>-0.734178</td>\n",
       "      <td>-1.148312</td>\n",
       "      <td>-0.738807</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>0.757373</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>2.276676</td>\n",
       "      <td>0.751056</td>\n",
       "      <td>3.289696</td>\n",
       "      <td>2.598314</td>\n",
       "      <td>-0.381482</td>\n",
       "      <td>-0.530613</td>\n",
       "      <td>0.809086</td>\n",
       "      <td>2.240762</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>0.757375</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>1.542902</td>\n",
       "      <td>0.272147</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>1.099199</td>\n",
       "      <td>-0.864188</td>\n",
       "      <td>2.580963</td>\n",
       "      <td>0.809088</td>\n",
       "      <td>1.613487</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.757376</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>-0.260958</td>\n",
       "      <td>-0.905172</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>1.099199</td>\n",
       "      <td>-0.864188</td>\n",
       "      <td>2.580963</td>\n",
       "      <td>0.809088</td>\n",
       "      <td>0.071428</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>0.757378</td>\n",
       "      <td>-0.185501</td>\n",
       "      <td>2.246102</td>\n",
       "      <td>0.731101</td>\n",
       "      <td>-0.321143</td>\n",
       "      <td>0.295115</td>\n",
       "      <td>1.075479</td>\n",
       "      <td>0.778314</td>\n",
       "      <td>0.809086</td>\n",
       "      <td>2.214626</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1602 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Invoice No  Line Item  Qty in Sales Unit  Sales Qty  Frieght Charged  \\\n",
       "0      -1.323463  -0.185501           1.390033   0.172374        -0.321143   \n",
       "1      -1.323462  -0.185501          -0.842688  -0.380625        -0.321143   \n",
       "2      -1.323461  -0.185501           1.390033   0.172374        -0.321143   \n",
       "3      -1.323455  -0.185501          -0.445869   0.858562        -0.321143   \n",
       "4      -1.323454  -0.185501          -0.230384  -0.885217        -0.321143   \n",
       "...          ...        ...                ...        ...              ...   \n",
       "1597    0.757371  -0.185501          -0.444218   0.863718        -0.321143   \n",
       "1598    0.757373  -0.185501           2.276676   0.751056         3.289696   \n",
       "1599    0.757375  -0.185501           1.542902   0.272147        -0.321143   \n",
       "1600    0.757376  -0.185501          -0.260958  -0.905172        -0.321143   \n",
       "1601    0.757378  -0.185501           2.246102   0.731101        -0.321143   \n",
       "\n",
       "      Freight Incurred  Interest  Net Value /KL (NET_VAL_KL)  \\\n",
       "0            -0.940121 -1.215172                   -0.555974   \n",
       "1            -0.544604 -0.595787                   -0.698871   \n",
       "2            -0.940121 -1.215172                   -0.555974   \n",
       "3            -0.219512 -0.305319                   -0.769007   \n",
       "4            -0.683575  1.014611                   -0.293881   \n",
       "...                ...       ...                         ...   \n",
       "1597          1.604958 -0.140149                   -0.734178   \n",
       "1598          2.598314 -0.381482                   -0.530613   \n",
       "1599          1.099199 -0.864188                    2.580963   \n",
       "1600          1.099199 -0.864188                    2.580963   \n",
       "1601          0.295115  1.075479                    0.778314   \n",
       "\n",
       "      Packing Cost /KL  Total Packing Cost  ...  Sales manager  Industry Code  \\\n",
       "0             0.814349            1.488775  ...            8.0            8.0   \n",
       "1            -1.148312           -0.738807  ...            9.0            7.0   \n",
       "2             0.814349            1.488775  ...            8.0            8.0   \n",
       "3            -1.148312           -0.738807  ...           10.0            7.0   \n",
       "4             0.826556            0.105028  ...            9.0            7.0   \n",
       "...                ...                 ...  ...            ...            ...   \n",
       "1597         -1.148312           -0.738807  ...           11.0            7.0   \n",
       "1598          0.809086            2.240762  ...           11.0            7.0   \n",
       "1599          0.809088            1.613487  ...            2.0            8.0   \n",
       "1600          0.809088            0.071428  ...            2.0            8.0   \n",
       "1601          0.809086            2.214626  ...           12.0            7.0   \n",
       "\n",
       "      Customer Grp  Industry Desc  MATKL  Mat Grp1 Text  Mat Grp 2 Text  \\\n",
       "0             54.0            9.0    0.0            0.0            14.0   \n",
       "1             57.0            8.0    0.0            0.0            14.0   \n",
       "2             54.0            9.0    0.0            0.0            14.0   \n",
       "3            183.0            8.0    0.0            0.0            14.0   \n",
       "4             37.0            8.0    0.0            0.0            14.0   \n",
       "...            ...            ...    ...            ...             ...   \n",
       "1597         290.0            8.0    0.0            0.0            14.0   \n",
       "1598          99.0            8.0    0.0            0.0            14.0   \n",
       "1599         133.0            9.0    0.0            1.0            19.0   \n",
       "1600         133.0            9.0    0.0            1.0            19.0   \n",
       "1601         268.0            8.0    0.0            1.0             9.0   \n",
       "\n",
       "       City  Doc Curr  Plant Desc  \n",
       "0     143.0       0.0         0.0  \n",
       "1     210.0       0.0         0.0  \n",
       "2     143.0       0.0         0.0  \n",
       "3     193.0       0.0         0.0  \n",
       "4       6.0       0.0         0.0  \n",
       "...     ...       ...         ...  \n",
       "1597   42.0       0.0         1.0  \n",
       "1598   97.0       0.0         1.0  \n",
       "1599   10.0       0.0         1.0  \n",
       "1600   10.0       0.0         1.0  \n",
       "1601  175.0       0.0         1.0  \n",
       "\n",
       "[1602 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.ingest_from_file('data/cleaned_apar.csv')\n",
    "data = processor.fit_transform(ingestor.get_data())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff9de5f-163a-49ab-a6f1-9fc2a4c1fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.clustering import ClusteringEngine\n",
    "\n",
    "engine = ClusteringEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdeb8a53-9e8b-43fc-96ef-23cf725928df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically determined task types:\n",
      "- Revenue: regression\n",
      "- Total Value: regression\n",
      "Processing 2 targets\n",
      "- Revenue: weight = 0.500\n",
      "- Total Value: weight = 0.500\n",
      "Computing feature importance for target: Revenue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiva/Desktop/Instinct_AI/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing feature importance for target: Total Value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiva/Desktop/Instinct_AI/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4/48 features\n",
      "- User-selected features: 4\n",
      "- SHAP-selected features: 0\n"
     ]
    }
   ],
   "source": [
    "clusters = engine.build_cluster_tree(df=data,kpi_columns=['Revenue','Total Value'],\n",
    "                          columns_to_analyze=['Commission_N','RM Cost /KL','NET_CONT_KL','Material'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74847501-a97f-40fe-822a-72b20c9e0d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NET_CONT_KL', 'Commission_N', 'Material', 'RM Cost /KL']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d44397c-bdf1-4034-9ea7-137d96113e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters.children[0].children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee063e9f-acfe-4b0b-b600-5dffbbd61877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a3da7-81da-458d-91a5-236a368ccb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b94a4-591b-4e46-ad38-d31a81275f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
